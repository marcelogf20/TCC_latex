%PORTUGUESE
\resumo{Resumo}{
	A compressão de imagens é um tópico exaustivamente estudado dentro da ciência, em busca de representar a informação de forma compacta. Esse esforço é justificado pelo grande tráfego dessa mída pela Internet, porém limitado pelos requisitos de largura de banda. Recentemente, pesquisas de algoritmos baseados em Redes Neurais Artificiais para compressão de imagens têm alcançado resultados com enorme potencial. Em particular, a abordagem de ponta a ponta na qual codificador e decodificador são treinados em conjunto  está presente nos trabalhos que superam os métodos tradicionais de compressão de imagens. 
	
	Essa abordagem é possível através de uma arquitetura de rede neural denominada autocodificador. Sob certas condições, ela é especialmente útil para aprender representações compactadas e bem estruturas dos dados. Na literatura, são relatados autocodificadores com características específicas para o problema de compressão. Dentre eles, muitos são baseados somente em camadas convolucionais, contudo esse método exige, geralmente, o treinamento de diversos modelos para diferentes taxas de compactação para que possam gerar uma curva taxa-distorção com boa pefomance.
	Nos autocodificadores com camadas recorrentes é possível gerar um modelo com flexibilidade para gerar taxas variadas com qualidade, ao aproveitarmos da sua capacididade para manter informações passadas. Os autocodificadores esparsos possuem alguma regularização envolvendo o espaço de código latente para encontrar represenações robustas dos dados.
	
	Introduzimos modificações em trabalhos anteriores baseados em redes neurais convolucionais recorrentes para compressão de imagem com perdas. Primeiro, avaliamos o papel de uma base de dados com diferentes característiacas de entropia na perfomance do modelo. Em seguida, foi feito uma análise empírica para obter uma função de custo adequada no nosso problema. 
	
	A introdução de um fator de regularização, interpretado aqui, como uma estimativa da entropia de ordem zero, nessa função propocionou ganhos consideráveis. Essa função teve o papel de promover, de certa forma, esparsidade no fluxo de bits gerados pelo codificador e mudar a frequência de ocorrência dos bits (-1 e 1 para o nosso modelo).  Essa estatística foi aproveitada pelo GZIP para reduzir as taxas. Por fim, propomos um esquema simples de alocação de bits para aproveita a diferença de complexidade nas regioões de uma imagem. 
	
	Os nossos resultados indicam melhor perfomance em relação ao JPEG nas medidas de PSNR, SSIM e MS-SSIM. Ao comparar esses resultados com o JPEG2000 obtemos desempenho competitivo. Contudo, o método de alocação dinâmica de bits teve o inconveniente de gerar artefatos de blocos visíveis e que degradam a qualidade perceptiva da imagem. 
\begin{singlespace}
{\setfonttimes\normalsize\noindent{\textbf{Palavras-chave:} \palavraschavecatalogoinome,~\palavraschavecatalogoiinome,~\palavraschavecatalogoiiinome,~\palavraschavecatalogoivnome.}}
\end{singlespace}
}
%ENGLISH
\vspace{2cm}
\resumo{Abstract}{


\begin{singlespace}
{\setfonttimes\normalsize\noindent{\textbf{Keywords:} \keywordiname,~\keywordiiname,~\keywordiiiname,~\keywordivname.}}
\end{singlespace}
}
