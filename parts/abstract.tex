%PORTUGUESE
\resumo{Resumo}{
	A compressão de imagens é um tópico exaustivamente estudado dentro da ciência, em busca de representar a informação de forma mais compacta possível. Esse esforço é justificado pelo grande tráfego dessa mídia pela Internet. Logo, qualquer redução nos dados economiza significativamente na largura de banda e no consumo de energia elétrica. Recentemente, pesquisas de algoritmos baseados em redes neurais artificiais para compressão de imagens têm alcançado resultados com enorme potencial. 
	Em particular, a abordagem ponta-ponta na qual o codificador e decodificador são treinados em conjunto por uma rede conhecida como autocodifcador  está presente na maioria desses trabalhos.
	
	
	Esta rede pode ser baseada somente em camadas convolucionais e exigem, geralmente, o treinamento de diversos modelos para comprimir a informação em diferentes taxas. Contudo, autocodificadores com redes recorrentes são úteis para obter um único modelo capaz de fornecer taxas progressivas, ao usar a capacidade dessas redes em manter informações passadas. Há ainda autocodificadores esparsos onde 
	apenas uma pequena parcela das saídas envolvidas na representação da informação é ativada, e a abordagem variacional que assume modelos probabilísticos para aprender representações latentes.
	
	Neste trabalho, realizamos modificações em trabalhos anteriores baseados em redes neurais convolucionais e recorrentes para compressão de imagem com perdas. 
	Primeiro, avaliamos bases de dados com diferentes características de entropia no desempenho do modelo. Em seguida, foram realizados testes com diferentes funções de custo até obtermos uma medida para otimizar a distorção e, de forma indireta, a taxa.
	O melhor desempenho ocorreu quando a distorção foi modelada pelo erro quadrático médio e a taxa pela quantidade de bits 1 presentes no código binário. Essa última função foi responsável por promover esparsidade no fluxo de bits, e permitir a codificação de entropia, fornecida pelo GZIP, redução considerável na taxa. Por fim, propomos um esquema simples de alocação de bits para aproveitar as diferenças de complexidades nas regiões de uma imagem. 
	
	Os nossos resultados indicam melhor desempenho em relação ao JPEG nas medidas de PSNR, SSIM e MS-SSIM. Ao comparar esses resultados com o JPEG2000 obtemos desempenho competitivo em altas taxas. Todavia, o método de alocação dinâmica de bits gera artefatos de blocos que degradam a qualidade perceptiva das imagens. 
	
	%Contudo, o método de alocação dinâmica de bits gera artefatos de blocos que degradam a qualidade perceptiva das imagens.foi usada pelo algoritmo do GZIP para reduzir as taxas de bitsEssa última função    
	%A taxa foi otimizada através da redução da frequência do bit 1 dentro do código binário.    Na prática, essa função promoveu esparsidade no fluxo de bits provenientes do codificador. A esparsidade da representação da informação foi usada pelo algoritmo do GZIP para reduzir as taxas de bits. Por fim, propomos um esquema simples de alocação de bits para aproveitar a diferença de complexidade nas regiões de uma imagem. Os nossos resultados indicam melhor performance em relação ao JPEG nas medidas de PSNR, SSIM e MS-SSIM. Ao comparar esses resultados com o JPEG2000 obtemos desempenho competitivo. Contudo, o método de alocação dinâmica de bits gera artefatos de blocos que degradam a qualidade perceptiva das imagens.
	

 
\begin{singlespace}
{\setfonttimes\normalsize\noindent{\textbf{Palavras-chave:} \palavraschavecatalogoinome,~\palavraschavecatalogoiinome,~\palavraschavecatalogoiiinome,~\palavraschavecatalogoivnome.}}
\end{singlespace}
}
%ENGLISH
\vspace{2cm}
\resumo{Abstract}{
	Image compression is a topic exhaustively studied within science, in order to represent information in the more compact way possible. This effort is justified by the great traffic of this media over the Internet. Therefore, any reduction in data saves significantly in bandwidth and electricity consumption. Recently, research of algorithms based on artificial neural networks for image compression has achieved results with enormous potential. In particular, the end-to-end approach in which encoder and decoder are trained together is present in works t traditional image compression methods. 
	
	In particular, the end-to-end approach in which encoder and decoder are trained together by a network known as autoencoder is present in most such works. This network can be based only on convolutional layers and generally requires training of various models to compress information at different compression ratios.	However, autoencoders with recurrent networks are useful for obtaining a single model capable of providing progressive rates by using the capacity of these networks to keep past information. There are also sparse autoencoder where only a small portion of the outputs involved in information representation are activated, and the variational approach that assumes probabilistic models for learning latent representations.
	


	In this work, we made modifications in previous works based on recurrent convolutional neural networks for lossy image compression. First, we evaluated different training databases with specific entropy characteristics in model performance.Then, tests with different cost functions were performed until we obtained a measure to optimize distortion and, indirectly, the rate. The best performance occurred when the distortion was modeled by the mean square error and the rate of bits equal to 1 present in the binary code.	This last function was responsible for promoting sparity bitstream, and thus allowing GZIP's entropy coding to considerably reduce at rate. 
	Finally, we propose a simple bit allocation scheme to take advantage of the complexity difference in the regions of an image. 
	
	Our results indicate better performance over JPEG on the PSNR, SSIM, and MS-SSIM measurements. By comparing these results with the JPEG2000 we get competitive performance at high rates. However, the dynamic bit allocation method generates block artifacts that degrade the perceptual image quality. 
	
	
	%In practice, this function promoted sparsity in the bit stream generated by the encoder. Thus, the frequency of occurrence of a given bit decreases and the frequency of the other increases. This statistic was used by the GZIP algorithm to considerably reduce bit rates. 
	%Finally, we propose a simple bit allocation scheme to take advantage of the complexity difference in the regions of an image. 
	
	%Our results indicate better JPEG performance on PSNR, SSIM and MS-SSIM measurements. By comparing these results with the JPEG2000 we get competitive performance. However, the dynamic bit allocation method generates block artifacts that degrade the perceptual image quality.


\begin{singlespace}
{\setfonttimes\normalsize\noindent{\textbf{Keywords:} \keywordiname,~\keywordiiname,~\keywordiiiname,~\keywordivname.}}
\end{singlespace}
}

	
%This approach is possible with a neural network called autoencoder. It is especially useful for learning compressed representations and well data structures. In the literature, projects of several autoenencoder architectures focused on the challenge of image compression are reported.
%Some are based only on convolutional layers and generally require the training of various models to compress at different compression ratios. 
%The	autoencoders with recurrent networks are useful for obtaining a single model capable of providing progressive rates.
%Sparse autoencoders have some regularization to find robust representations of the data. The variational autocoder assumes probability models for learning latent representations.
