\chapter{Revisão Bibliográfica}

Neste capítulo, apresentamos uma revisão bibliográfica de \textit{codecs} de imagem desenvolvidos com técnicas de redes neurais artificiais. O autocodificador é o tipo de RNA presente em todos os trabalhos. As diferenças estão na arquitetura dos autocodificadores, na modelagem do problema, emprego de análises probabilísticas, generativas, de segmentação semântica etc.  

O problema central na compressão de imagens com perdas é a otimização conjunta da taxa e distorção. Esse objetivo se torna intratável para imagens ou espaços de alta dimensão sem a imposição de alguma restrição \cite{gersho2012vector}. 

%Otimização conjunta de taxa e distorção é difícil. Sem maiores restrições, o problema geral da quantização ótima em espaços de alta dimensão é intratável (
%Os codecs artesanais tradicionais carecem de adaptabilidade e são incapazes de aproveite a redundância semântica em imagens naturais. 

Devido a isso, os \textit{codecs} tradicionais aplicam transformações lineares para gerar uma representação adequada sobre a qual podemos considerar a otimização conjunta de distorção e taxa \cite{End2016Balle}.  Por exemplo, o JPEG realiza a DCT e a decomposição ortogonal em várias escalas de \textit{Wavelets} é usada pelo JPEG2000.

Entretanto, não há razão para esperar que uma função linear seja ideal para comprimir todo o espectro de imagens naturais e levando em consideração os formatos de mídia emergentes \cite{santurkar2018generative}.  Uma alternativa é substituir as transformações lineares por redes neurais artificiais a fim de obter um \textit{codec} que generaliza melhor sobre as imagens. 


Há 3 principais abordagens para incluir RNAs na tarefa de compressão de imagens. A primeira é o desenvolvimento completo do par codificador-decodificador. A segunda consiste na implementação de redes neurais dos algoritmos de compactação de imagem tradicionais. A terceira abordagem é usar RNA para substituir, aperfeiçoar ou adicionar alguma etapa na compressão de imagens realizadas por codecs convencionais. Aqui o objetivo dela é fornecer melhorias adicionais a esses algoritmos \cite{Jiang1999}. Hoje, a primeira abordagem é a predominantes e vem obtendo os melhores resultados.  

As técnicas de redes neurais para compressão de imagens é um tema que vem despertando o interesse de muitos pesquisadores e já apresenta resultados competitivos com o BPG (sigla do inglês \textit{Better Portable Graphics} e considerado o estado da arte de compressão de imagens) em alguns trabalhos \cite{Autoregressive2018Minnen,akbari2019dsslic}. 

%Uma das vantagens do uso de RNAs para a compressão de imagens é a facilidade de adaptação da rede para que possa comprimir eficientemente novas mídias (imagens não-naturais).  %É razoável pensar no uso de IA para tarefa de compressão de imagens.

A natureza da compressão com perdas pressupõe a utilização de quantização dos dados. Tal operação não é diferenciável e impõe uma restrição ao uso de RNA para a compressão. Outra preocupação é fornecer ao codec flexibilidade similar aos codecs tradicionais de imagens. Em geral, deseja-se que a rede seja capaz de fornecer reconstruções progressivas, controlar a qualidade e a taxa, codificar e decodificar  em tempo similar os codecs convencionais etc.

%Na literatura, os autocodificadores se popularizaram como uma rede neural capaz de realizar compressão de dados.  Como já apresentado é uma abordagem fim-a-fim que pode ser adaptada para as tarefas de compressão de imagens.


\section{Autocodifcadores em cadeia}

Em \cite{Variable2016Toderici}, Toderici et. al propuseram arquiteturas ou modelos de redes neurais voltados para compressão imagem em miniaturas. As arquiteturas são baseadas no uso de autocodificadores encadeados e treinadas pelas imagens e suas informações residuais obtidas progressivamente. 
Esse encadeamento pode ser explícito, isto é, todos os autocodificadores foram explicitamente definidos em termos de programação e, portando, cada um possui seus próprios pesos e camadas. 
No encadeamento implícito apenas um autocodificador é projetado, porém durante a execução do modelo conexões de retroalimentação são realizadas nesse autocodificador. Logo, aqui temos um cascateamento em laço onde os pesos do autocodificador são compartilhados nas iterações. O número de autocodificadores (implícitos ou explícitos) define o número de iterações do nosso modelo. 

Durante a execução (treinamento ou teste) de qualquer uma das arquiteturas apresentadas em \cite{Variable2016Toderici} a primeira iteração segue o seguinte procedimento: toma uma imagem $x = r_0$ passa pela função $E_1$ (codificador 1), o resultado é transformado em códigos binários por uma função $B$ (função de binarização), em seguida a rede do decodificador $D_1$ (decodificador 1) cria uma estimativa da imagem de entrada original com base no código binário recebido. Então, calcula-se o resíduo da primeira iteração, dado por $r_1 = r_0 - D_1(B(E_1(r_0)))$. 
Na próxima iteração, o $E_2$ recebe a informação residual e a partir daqui realiza o mesmo procedimento que a primeira iteração, entretanto o resultado gerado pelo $D_2$ (e das próximas iterações) pode ser a reconstrução da informação residual (reconstrução aditiva) ou a reconstrução de uma nova versão da imagem original (reconstrução única), como veremos.  Após as $k$ iterações, passamos a processar a próxima imagem e o ciclo se repete. 

Podemos resumir a ideia do encadeamento de autocodificadores com a próxima Equação: 

\begin{equation}
\begin{aligned}
F_t(r_{t-1}) &= D_t(B(E_t(r_{t-1}))) \\
\end{aligned}
\end{equation}

onde $E_t$ e $D_t$ são o codificador e decodificador da iteração $t$-ésima para a qual $ r_{t-1}$ é a entrada. A função de binarização $B$ é o mesmo em todas as iterações. A equação para obter o resíduo $r_t$ depende da informação de saída do autocodificador $F_t$.  Os pares codificador-decodificador são treinado de ponta a ponta, mas durante a implantação, eles são normalmente usados independentemente.

Antes de apresentar os modelos com mais detalhes, discutiremos o processo de binarização adotado.


\subsection{Binarização}
\label{subsec:bin}

%A binarização consiste em um mapeamento de valores em ponto flutuante para o intervalo $[- 1, 1]$. Ela é equivalente a função de ativação limiar descrita na Tabela \ref{table:func_ativacoes}. Logo, tal operação possui derivada nula ou indefinida.  Por isso, é necessário uma manipulação para que possam treinar uma rede neural com uma camada de binarização.
%A primeira parte é a geração de valores no intervalo contínuo $[- 1, 1]$. A segunda parte envolve a conversão desses valores para o conjunto discreto $\{- 1, 1\}$.


O trabalho \cite{Variable2016Toderici} propõe uma operação de binarização e um método para transpor a limitação que esse processo impõe ao treinamento de uma RNA. 
Nessa proposta, uma camada totalmente conectada com ativação de $\tanh$ é usada para produzir as saídas no intervalo contínuo $[1,-1]$. Em seguida, uma abordagem estocástica usada durante o treinamento da rede é aplicada para converter desses valores para o conjunto discreto $\{- 1, 1\}$. Sendo $x$ um valor a ser quantizado e $\epsilon$ o erro de quantização devemos ter.

\begin{equation}
\begin{aligned}
b(x) &= x + \epsilon  \\
b(x) & \in \{-1, 1\}
\end{aligned}
\end{equation}

O valor $\epsilon$ é obtido dentre dois valores possíveis e obedece a seguinte regra estocástica:

\begin{equation}
\label{eq:quant2}
\begin{aligned}
\epsilon \sim \left\{
\begin{array}{ll}
1 - x, \text{com probabilidade } \frac{1 + x}{2} \\
-x - 1, \text{com probabilidade} \frac{1 - x}{2}
\end{array}
\right. \\
\end{aligned}
\end{equation}

Na prática, definimos um valor $u$ entre 0 e 1 obtido de uma distribuição uniforme, isto é,  $u \in \mathcal{U}[0,1]$ e podemos reescrever \ref{eq:quant2} como:

\begin{equation}
\label{eq:quant}
\begin{aligned}
\epsilon \ = \left\{
\begin{array}{ll}
1 - x, \text{ se $u$}  \leq \frac{1 + x}{2} \\
-x - 1, \text{ se $u$} > \frac{1 + x}{2}
\end{array}
\right. \\
\end{aligned}
\end{equation}


A função de binarização $B$ é dada por: 
\begin{equation}
B(x) = b(\tanh{W^{bin} + b^{bin}})
\end{equation}

onde, $W^{bin}$ e $b^{bin}$ são os pesos e viés lineares da camada imediatamente anterior à binarização. Para permitir a retropropagação do erro pela rede, substituímos o gradiente da binarização (que possui valor nulo ou indefinido) pelo gradiente da expectativa da binarização:

\begin{equation}
\begin{aligned}
\displaystyle \frac{\partial \mathbf{E}[b(x)]}{\partial x} = \displaystyle \frac{\partial x}{\partial x} =1,  \; \forall x \in [-1, 1] 
\end{aligned}
\end{equation}

Esse resultado significa que o algoritmo de retropropagação ignora a operação de binarização no cálculo do gradiente de todos os pesos da rede. 
Durante o teste do modelo (após o seu treinamento) a binarização é simplificada pelo resultado mais provável de $b(x)$:

\begin{equation}
b^{inf}(x) = \left\{
\begin{array}{ll}
-1 \text{, se } x < 0, \\
+1 \text{, caso contrário}.
\end{array}
\right. 
\end{equation}

\subsection{Redes não-recorrentes}

Seguindo o trabalho \cite{Variable2016Toderici}, quando o modelo é composto apenas por camadas não-recorrentes o encadeamento de autocodificadores é explícito e $F_t$ sempre fará uma estimativa do resíduo. Logo:


\begin{equation}
\label{eq:reconst_escalar}
\begin{aligned}
r'_{t}  &= F_t(r_{t}) //
r_{t+1} &= r'_{t} - r_{t}
\end{aligned}
\end{equation}

No primeiro modelo concebido, as redes do codificador e decodificador são formadas por camadas totalmente conectadas com 512 unidades (neurônios) exceto a última camada do codificador (às vezes denominado de camada de binarização) que é formada por 4 neurônios. Assim, a rede fornece um vetor latente de 4 bits por iteração. A reconstrução da imagem original é obtida progressivamente pela soma dos resíduos gerados, conforme Equação a seguir.

\begin{equation}
x' = \sum_{t=0}^{k-1} r'_{t}
\end{equation}


A Figura  \ref{fig:toderici1} ilustra tal arquitetura em duas iterações e omitindo a última camada do decodificador. Ela é responsável por converter os dados da camada anterior para uma imagem RGB de mesma dimensão que a entrada.


%Para $m$ iterações na rede, termos $4 \times m$ bits gerados.  

%Considerando 16 iterações, o número total de bits é 64. Se a entrada é uma imagem de $32 \times 32$ pixels chega-se a taxa 0,0625 bpp. A figura \ref{fig:toderici1} apresenta tal arquitetura com 2 autocodificadores. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/toderici_1.pdf}
	\caption{Um autoencoder residual totalmente conectado com  função de ativação $\tanh$. Esta figura representa uma arquitetura de duas iterações. As primeiras iterações codificam a imagem original. Os resíduos da reconstrução são passados para a segunda iteração. Cada nível de iteração produz 4 bits \cite{Variable2016Toderici}.}
	\label{fig:toderici1}
\end{figure}


Os autores estenderam essa arquitetura substituindo as camadas totalmente conectadas pelos operadores de convolução no codificador $E$ e pelos operadores ``deconvolucionais'' ou de convolução transposta no decodificador $D$. Consulte \cite{Variable2016Toderici} para obter a formulação matemática da convolução transposta. Pela própria definição da Conv2D a taxa gerada na binarização é independente do tamanho da imagem de entrada. Os autores definiram essa taxa a 2 bits por pixel por iteração. A figura \ref{fig:toderici2} mostra essa arquitetura (exceto a conversão RGB). 


\begin{figure}[h]
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/toderici_2.pdf}
	\caption{O codificador residual convolucional/deconvolucional. As camadas
		convolucionais são representadas como retângulos pontiagudos, enquanto as
		camadas deconvolucionais são representadas como retângulos arredondados. \cite{Variable2016Toderici}.}
	\label{fig:toderici2}
\end{figure}

\section{Redes Recorrentes}

%Muitas implementações de camadas foram propostas para manter a informação temporal. Uma das primeiras abordagens eficazes baseiam-se na Long Short-Term Memory (LSTM). Aprimoramentos posteriores incluem a Unidade Recorrente Gated (GRU), que simplifica o componente recorrente ao mesmo tempo em que obtém desempenho similar ao LSTM em alguns cenários \cite{NeuralReview2019Siwei}.


Em autocodificadores recorrentes há a noção de memória entre os estágios de tempo. Essa característica permite a função  $F_t$  estimar diretamente a imagem original em cada estágio $t$, isto é, $F_t$($r_{t-1}$) = $x'_{t}$ ou obter a reconstrução dos resíduos por iteração como apresentado na Equação \ref{eq:reconst_escalar}. 
Nessas abordagens o encadeamento de autocodificadores é implícito (uso recursivo do autocodificador). A motivação de usar camadas recorrentes foram utilizados para capturar as dependências entre os resíduos gerados em cada iteração. 

Ainda no trabalho \cite{Variable2016Toderici}, explorou-se o uso de modelos recorrentes em duas arquiteturas. Na primeira, o codificador é composto por uma camada totalmente conectada seguida por duas camadas LSTM empilhadas. O decodificador tem a estrutura oposta: duas camadas LSTM empilhadas seguidas por uma camada totalmente conectada com uma não-linearidade que prediz os valores RGB. Aqui, o modelo da LSTM segue a formulação da Equação \ref{eq:lstm}.

A Figura \ref{fig:toderici_ae_lstm} em mostra um desenrolar do LSTM (menos a conversão RGB), necessária para treinamento, em duas etapas de tempo. As conexões verticais entre os estágios LSTM no desenrolar mostram o efeito de propagação de memória. 
Na segunda abordagem, os autores combinam os operadores convolucionais e deconvolucionais com a LSTM. Tal arquitetura obteve a melhor performance dentre os testes realizados. %Entraremos em detalhes dessa formulação.  


\begin{figure}[h]
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/ae_lstm.pdf}
	\caption{O codificador residual LSTM totalmente conectado.  Esta figura mostra um desenrolamento do LSTM, necessário para o treinamento, em duas etapas.  Ao todo, a rede foi treinada com 16 níveis de resíduo, para gerar representações de 64 bits. As conexões verticais entre os estágios LSTM no desenrolamento mostram o efeito da memória persistente. Nessa arquitetura LSTM, cada etapa prevê a saída real. \cite{Variable2016Toderici}.}
	\label{fig:toderici_ae_lstm}
\end{figure}

Nesse trabalho, os autores usaram um banco de dados de referência com 
216 milhões de imagens coloridas e coletadas aleatoriamente da internet. Essas imagem foram reduzidas para blocos de $32 \times 32$ que elimina artefatos de compactação preexistentes para a maioria das imagens \cite{Variable2016Toderici}. Para o treinamento dos modelos LSTM, 90\% das imagens foram utilizadas; os restantes 10\% foram reservados para avaliação.  A função de custo utilizada foi a métrica $L_2$ nos resíduos. 

Ela é definida substituindo $p=2$ na Equação a seguir \cite{nie2010efficient}:
\begin{equation}
||v||_p = \left ( \sum_{i=1}^n |v_i|^p \right)^{\frac{1}{p}}  
\end{equation}

onde, $v \in \mathbb{R}^n$ é um vetor sobre o qual estamos aplicando a norma $L-p$. 



%Uma rede neural recorrente RNR é um tipo de rede neural com memória para armazenar informações sobre entradas passadas. 
%Uma unidade recorrentes são úteis para processamento de sequências em que a predição atual depende de informações passada. Para que isso seja possível, essa unidades possui uma estrutura em cadeia de modo que sua saída anterior $h_{t-1}$  é usada como parte da entrada no novo processamento - são conexões consigo mesmas. 




%A camada LSTM possui a seguinte formulação, em que $ x_t $, $ c_t $ e $ h_t $ indicam os estados de entrada, célula e ocultos na iteração $ t $ \ citeC{FullResolution2017Toderici}:

%\begin{equation}
%	\begin{aligned}
%	{[f, i, o, j]}^T & = [\sigma, \sigma, \sigma, \tanh]^T((Wx_t + Uh_{t-1}) + b) \\
%	c_t & = f \odot c_{t-1} + i \odot j \\
%	h_t & = o \odot \tanh(c_t)
%	\end{aligned}
%\end{equation}
%onde $\odot $ é a multiplicação de elementos, $b$ é o bias e $\sigma$ é a função sigmóide. A saída da camada é $h_t$. 


%Duas arquiteturas propostas em \cite{Variable2016Toderici} utilizam a abordagem recorrente. Na primeira arquitetura, E e D consistem de camadas LSTM empilhadas além de camadas totalmente conectadas. A figura em mostra um desenrolar do LSTM, necessário para treinamento, em duas etapas de tempo. As conexões verticais entre os estágios LSTM no desenrolar mostram o efeito de propagação de memória. 
%Observe que, em contraste com a Figura 1, em que a rede após a primeira etapa é usada para prever o erro residual da etapa anterior, nessa arquitetura LSTM, cada etapa prevê a saída real.

%Na segunda abordagem os autores combinam os operadores convolucionais e desconvolucionais com o LSTM. Definiu-se o LSTM convolucional substituindo a transformação Tl 4n na equação (8) por convoluções mais vieses. Então a função de transformação para LSTM convolucional com passo k é
%Aqui usamos os índices c e d para diferenciar os pesos associados às operações de convolução e deconvolução. Para construir o decodificador deconvolucional de LSTM, substituímos a segunda e terceira camadas deconvolucionais do decodiftexicador deconvolucional da Figura 3 por LSTM deconvolucional.

Em \cite{FullResolution2017Toderici} foi proposto um modelo convolucional-recorrente como resultado de melhorias do trabalho em \cite {Variable2016Toderici}. A ideia central permanece a mesma: encadeamento de autocodificadores treinados sobre os resíduos e com taxa de compressão fixa por iteração.  Tal arquitetura é representada compactamente através da Equação \ref{eq:ae_full}. 

\begin{equation}
\label{eq:ae_full}
\begin{aligned}
r_{0} &=x\\
b_{t} &= B(E_{t}(r_{t-1})) \\
\hat{x}_{t} &= D_{t}(b_{t}) + \gamma \hat{x}_{t-1} \\
r_{t} &= x- \hat{x}_{t}
\end{aligned}
\end{equation}

onde $E_t$, $D_t$, $b_t$, $\hat{x}_t$ e $r_t$ representam, respectivamente, codificador, decodificador, fluxo de bits, reconstrução progressiva da imagem original $x$, e o residual entre $x$ e $\hat{x}_t$ na iteração $t$. Se o valor de $\gamma$ é zero a reconstrução é do tipo única (``um disparo''), caso contrário $\gamma = 1$  e teremos a reconstrução aditiva. Na reconstrução única cada iteração sucessiva tem acesso a mais bits gerados pelo codificador, o que permite uma melhor reconstrução. 

%Em cada iteração, a função $B$ produzirá $m$ bits pertencente ao conjunto ${-1,1}^m$.  Após k iterações, a rede produz $m x k$ bits no total.  m é uma função linear do tamanho da entrada. Para patches de imagem de 32x32, m = 128.

O modelo é composto por camadas convolucionais bidimensionais (Conv2D); hibridas do tipo convolucional-recorrente (ConvRNN); e camadas estáticas de binarização e profundidade para espaço (PE). A figura \ref{fig:toderici3} apresenta um fluxograma da arquitetura para uma iteração.


As camada ConvRNN seguem formulação matemática dependente do tipo de rede recorrente. 
Em um dos modelos, os autores empregam a Conv2DLSTM descrita pela Equação \ref{eq:conlstm}. Também foram testadas a LSTM associativa e a Unidades Recorrentes Fechadas do inglês \textit{Gated Recurrent Units} (GRU).
Independentemente da camada recorrente dentre as 3 citadas, as RNNConv
incluem duas convoluções 2D: uma no vetor de entrada, $x_t$, outra no vetor de estado oculto anterior, $h_{t-1}$, que será referida como ``convolução oculta'' realizada pelo  ``filtro oculto''. Os resultados das convoluções são somados e seguem para uma etapa de processamento (dependente da rede recorrente) para fornecer $h_t$ e $c_t$.

A PE interpola os \textit{pixels} em diferentes canais para formar imagens de alta resolução de acordo com fator de escala, $upscale\_factor$. Essa operação também é conhecida como \textit{PixelShuffle}. Para exemplificar, seja uma entrada $X$ e fator de escala $uf$, então obteremos a saída $Y$ segundo:   

\begin{equation}
\label{eq:pf}
\begin{aligned}
dim(X) &= (N,L,H_{in}, W_{in}) \\
L &=  C \times \text{uf}^2 \\ 
Y &= pixel\_shuffle(X, \text{uf}) \\
dim(Y) &= (N,C,H_{in} \times \text{uf}, W_{in}\times \text{uf}) 
\end{aligned}
\end{equation} 

onde $dim \left( \odot \right)$ retornar a dimensão de um tensor. A PE reduz o número de canais e aumenta a dimensão espacial, por isso pode ser usada em substituição a convolução transposta.  

\begin{figure}[h]
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/toderici_3.pdf}
	\caption{The flow of the iterations in an RNN architecture. In this case, the prefix $D-$ refers to the "inverse" operation in the decoder.   Also, the depth to space is a shuffle of the pixels as defined in the sub-pixel operations \cite{FullResolution2017Toderici}.}
	\label{fig:toderici3}
\end{figure}

%Existem algumas diferenças de arquitetura, embora sejam semelhantes em um contexto geral.


Durante o treinamento dos modelos em \cite{FullResolution2017Toderici} uma perda ponderada de $L_1$ é calculada sobre os resíduos gerados em cada iteração, isto é, a perda total é $\beta \sum_t|r_t|$. Apenas ao final das iterações de recorrência é que ocorre a atualização dos pesos. 
Para treinar o modelo foram usados dois conjuntos de imagens de $32 \times 32 \times 3$. O primeiro corresponde a base de dados usados em \cite{Variable2016Toderici}.
Para obter o segundo conjunto de dados, primeiro reuniu-se uma amostra aleatória de 6 milhões de imagens de $1280 \times 720$ da \textit{Web} que foram decompostas em blocos em $32 \times 32 \times 3$ sem sobreposição  e codificadas com PNG. Então os 100 blocos de mais alta entropia por imagem, isto é, com a pior taxa de compactação, foram novamente amostrados e reunidos no segundo conjunto de dados. A partir daqui diversas melhorias foram propostas na tentativa de contornar limitações impostas pelo modelo. 

Ainda, em \cite{FullResolution2017Toderici} os autores usam uma rede neural específica, $G$,  para promover um fator de ganho nos resíduos. A saída dela é condicionada pela reconstrução e iteração, isto é, $g_t = G(x'_t)$. A intenção é impedir que o erro residual se torne ínfimo a ponto de comprometer a convergência do modelo. O fluxo da informção usando essa rede de ganho é mostrado na Figura \ref{fig:toderici4}.

Outra melhoria proposta foi a adição de uma nova RNA de codificação de entropia para melhorar a taxa de compactação. De forma simplificada, é empregado uma rede recorrente binária, baseada em uma LSTM, que induz um modelo de probabilidade a um codificador aritmético.





%Uma das principais desvantagens das propostas de ponta a ponta é a não flexibilidade das redes, como mencionado anteriormente. Essa rigidez impõe uma restrição na variabilidade do número de bits aplicado a cada patch. \cite{FullResolution2017Toderici} também tenta resolver esse problema pelo menos no contexto da codificação de entropia. 

%Assim, os autores propõem uma rede recorrente binária, baseada em um LSTM, que induz um modelo de probabilidade a um codificador aritmético. A figura \ref{fig:toderici5} representa a modelagem referida

%\begin{figure}
%	\centering
%	\includegraphics[width=0.90\textwidth]{figuras/toderici_4.pdf}
%	\caption{Using a content-dependent residual to the framework. This scalar gain factor is applied to all color channels \cite{FullResolution2017Toderici}.}
%	\label{fig:toderici4}
%\end{figure}

%Essa arquitetura é baseada no PixelRNN e usa uma linha de estados $S$ de tamanho $1 \times W \times k$ para capturar dependências de curto e longo prazo para modelar probabilidades de código. Uma convolução mascarada de $7 \times 7$ é aplicada e produz um $z_0$. Este LSTM processa uma linha de varredura por vez. A última etapa é composta de convoluções de $1 \times 1$ para aumentar a capacidade de memorização da rede.

%\begin{figure}
%	\centering
%	\includegraphics[width=0.70\textwidth]{figuras/toderici_5.pdf}
%	\caption{BinaryRNN \cite{FullResolution2017Toderici}.}
%	\label{fig:toderici5}
%\end{figure}

%A arquitetura ponta a ponta apresenta a vantagem de controle na taxa de bits que está relacionado com o gargalo da rede. As reconstruções progressivas são obtidas em virtude do uso da informação residual. Contudo, ela apresenta as seguintes desvantagens:

%\begin{itemize}
%    \item Consumo de muito recurso computacional 
%    \item Tempo para codificar e decodificar uma imagem em modelo treinadao
%    \item Uso do mesmo número de bits para comprimir patches simples (baixa entropia) e complexos (alta entropia)   
%    \item Rigidez para controlar a qualidade da imagem através de alguma métrica alvo 
%\end{itemize}


%O treinamento foi realizado com o otimizador Adam com taxas de aprendizado de $\{0.1, 0.3, 0.5, 0.8, 1\}$ e uma perda normalizada de $ L_2 $. O número de iterações variou de 8 a 16. O SSIM é usado para avaliar o desempenho em tempo de teste. Vale a pena notar que esta solução introduz artefatos de codificação em limites de patch.


%Em trabalhos posteriores houve atenção especial para otimizar o modelo apresentado em \ref{fig:toderici3}. 

Em \cite{target} os autores modificaram a rede \ref{fig:toderici3} 
para usar 0/1, em vez de -1/1 a fim de evitar artefatos de limite nas bordas das imagens. Além disso, eles implementaram um método para que a rede possa alocar mais bits nos blocos mais difíceis de reconstruir em comparação com os mais simples. 
Para que isso fosse possível uma camada com máscara é utilizada sobre o código binário gerado e aplicado após a primeira iteração da rede, no tempo de treinamento.
O mascaramento pode ser transparente sobre esse código binário e, nesse caso, a saída dessa camada coincide com o código binário. Contudo, se em uma dada iteração da rede pelo menos uma das seguintes condições forem satisfeitas, a máscara levará todos os bits para zero. 

\begin{itemize}
	\item  Onde a qualidade de reconstrução do bloco excede nosso nível de qualidade desejado;
	\item A saída do codificador é acidentalmente zero;
	\item Os códigos foram mascarados em uma iteração anterior.
\end{itemize}

Essa lógica de mascaramento permite tratar um código de bits zero como um sinal de parada e evitar o envio de qualquer informação subsequente do codificador para o decodificador.   

Neste trabalho, outra mudança atingiu a função de perdas da rede. Para essa função, foi considerada a perda $L_1$ sobre os resíduos (erro de reconstrução) e uma função (não detalhada no referido artigo) proporcional a quantidade de bits diferente de zero. O propósito dessa segunda parcela  é tornar o código binário com maior taxa de 0 e possibilitar compactação de entropia mais eficaz. A codificação de entropia usada foi a LempelZiv (LZ77). 
Os codecs de imagens que apresentam melhor desempenho se valem do conteúdo espacial da imagem para alocar os bits disponíveis de forma otimizada. Tal técnica é empregada, por exemplo, pelo JPEG2000, WebP e BPG \cite{boliek2000information}.

No trabalho apresentado por Minnen et. al. em \cite{SpatiallyAdaptive2018Minnen} é discutido um método para implementar uma alocação de bits com base na complexidade de regiões de imagens. Na prática, essa adaptação da taxa se dá pela correlação entre os blocos próximos de uma imagem. 

%Aqui, essa tarefa ocorrerá em tempo de treinamento para promover adaptação na taxa de bits com base na . 

Aqui, o codec proposto é composto por duas redes neurais para realizar tarefas específicas. A primeira rede deverá gerar previsões de blocos da imagem a partir dos blocos vizinhos, a segunda recebe o resíduo dessa previsão para reconstruí-lo progressivamente. 
Ao final, a reconstrução da imagem corresponde a primeira previsão somada aos resíduos reconstruídos. A primeira rede é um autocodificador convolucional, ilustrado na Figura \ref{fig:toderici8}, que recebe 1 bloco $64 \times 64$ ou, de forma equivalente, 4 blocos $32 \times 32$, sendo 3 deles da imagem original e um bloco com pixels zeros. Este último é o alvo que a rede tentará prever na saída ao minimizar o erro $L_1$ entre essa previsão e o bloco original.  A figura \ref{fig:toderici7} ilustra o processo de previsão descrito até aqui.     


\begin{figure}
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/toderici_8.pdf}
	\caption[Modelo DPCM com autocodificador]{A rede DPCM é um codificador automático que recebe uma imagem de $64 \times 64$, com zeros no bloco atual e gera esse bloco ausente\cite{SpatiallyAdaptive2018Minnen}.}
	\label{fig:toderici8}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/toderici_7.pdf}
	\caption[]{Para cada  $32 \times 32$, o modelo DPCM tenta prever com base nos blocos vizinhos. Os resíduos são propagados para o autocodificador \cite{SpatiallyAdaptive2018Minnen}.}
	\label{fig:toderici7}
\end{figure}

A segunda rede, irá complementar a tarefa da primeira ao reconstruir a informação residual. Ela corresponde a rede \ref{fig:toderici3} com reconstrução aditiva e usando unidades Con2DLSTM. 

%“LSTM (Additive Reconstruction)” apresentada por Toderici et al. [2], exceto que onde esse papel treina a rede para compactar imagens completas, treinamos para compactar o resíduo em cada bloco após executar o preditor de contexto.


No artigo \cite{Priming2017Johnston}, Johnston et al. implementaram 3 modificações na forma de treinar o modelo que o fez superar os codecs BPG(4:2:0), WebP, JPEG2000 e JPEG medidos na métrica MS-SSIM. 

A primeira mudança foi na execução de iterações falsas na rede (passos adicionais) a fim de inicializar os estados ocultos das camadas recorrentes ($h_t$ e $c_t$) com valor diferente de zero - essa técnica foi chamada de \textit{k-priming} de estado oculto.
Nessa técnica, um lote de imagens passa k vezes pela rede do codificador descartando os bits gerados nas primeiras k-1 vezes (falsas iterações). Todavia as alterações nos tensores de estado oculto das unidades recorrentes do codificador são mantidas. Quando o lote passa pela k-ésima vez pelo codificador teremos uma iteração real e os bits são transmitidos para o decodificador. 
De forma análoga, na rede do decodificador esses bits são usados para reconstruir k vezes o conjunto de imagens. Dessa forma, a rede consegue inicializar os estados ocultos do decodificador e na iteração k a reconstrução será válida para continuar o treinamento.    

O \textit{k-priming} pode ser executado, também, entre as iterações e é especificamente denominado por \textit{k-difusão}. Essa técnicas proporcionaram melhores resultados, mas ao custo de exigir mais tempo de execução e recursos computacionais \cite{Priming2017Johnston}. 

%Tanto no \textit{k-priming} quando na \textit{k-difisão}  à medida que aumentamos k, aumentamos o suporte máximo do sistema junto com o tempo de computação e treinamento.

Como foi discutido a abordagem de redes neurais em cascata adiciona uma quantidade fixa de bits em cada iteração. Esse fato torna a rede ineficiente na medida que, independentemente da complexidade da imagem, o mesmo número de bits são utilizados para representá-la.  Por exemplo, para representar partes da imagens de baixa frequência (por exemplo o céu sem nuvens) são necessários menos bits que informações de alta frequência (por exemplos confetes).

A solução proposta pelos autores em \cite{Priming2017Johnston} é, dada uma qualidade de destino, atribuir a cada bloco de $16 \times 16$ pixels uma quantidade de bits suficientes para atender ou exceder a qualidade de destino até o máximo suportado pelo modelo.
Esse processo ocorre no tempo de teste da rede (após o seu treinamento) e algumas  heurísticas foram adotadas para evitar artefatos de compressão. O bloco pode ser subdividido em 4 blocos  $8 \times 8$, sobre os quais obtém-se a norma $L_1$ do erro residual. A qualidade do bloco $16 \times 16$ será tomado como o valor máximo dentre as 4 medidas de erro. Cada bloco $16 \times 16$ usará entre $50\%$ e $120\%$ da taxa de bits de destino (arredondado para a iteração mais próxima). 

O codificador terá que criar uma matriz para indicar quantos bits foram reservados para cada localização da imagem. O algoritmo do decodificador requer que todos os bits estejam presentes para reconstruir uma imagem, logo ele preenche com um valor numérico as regiões nas quais o codificador reservou menos bits que o máximo possível. O valor adotado pelos autores foi 0, o valor médio da binarização (-1 e 1). 


Por fim, a terceira mudança recaiu na função de custo. Os autores adotaram uma função para considerar, além do erro tradicional de $L_1$, uma métrica perceptual de qualidade. Ela corresponde a métrica $L1$ ponderada por uma função que mede a dissimilaridade perceptual entre duas imagens \ref{eq:loss}.    

\begin{equation}
\label{eq:loss}     
\begin{aligned}
L(x,x') &= w(x,x') \times L_1(x-x') \\ 
w(x,x') &= \frac{S(x,x')}{\bar{S}} 
\end{aligned}
\end{equation}

onde $S(x, x')$ é uma medida perceptual da dissimilaridade entre as imagens $x$ e $x'$ e  $\bar{S}$ é ajustada para a média móvel de $S(x,x')$. 
Intuitivamente, essa perda está realizando amostragem de importância dinâmica: compara a distorção perceptual de uma imagem com a distorção perceptiva média e pesa mais as imagens com alta distorção perceptual e menos pesadamente as imagens para as quais a rede de compressão já funciona bem.
Na prática, $S(x, x')$ foi modela pela medida de dissimilaridade $D(x, x') = 0,5 \times (1- SSIM (x, x'))$. 
A Tabela \ref{table:codecs} apresenta as principais características dos autocodificadores em cadeia relatos nesse trabalho.  

%Na prática, a imagem é dividida em blocos $8 \times 8$. Para cada bloco é calculado um valor de ponderação local, dado pela dissimilaridade: $D(x, x') = 0,5 \times (1- SSIM (x, x'))$. A perda em toda a imagem, $L(x,x')$, é igual a soma de $D(x,x')$ ponderadas localmente por $L_1$.


\begin{landscape}
	
	\begin{table}[htbp]
		\caption[Tabela com resumo de codecs baseados em autocodificadores em cadeia.]{Tabela com informações sintetizadas do \textit{codes} de imagens baseados em autocodificadores em cadeia.}
		\begin{adjustbox}{width=\columnwidth,center}
			\begin{tabular}{|l|l|l|l|l|l|}
				\hline
				\rowcolor[HTML]{EFEFEF} 
				\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}Autores}  & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Autocodificador}                                                                                                                 & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Descrição}                                                                                                                                                                                         & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}
					\begin{tabular}[c]{@{}c@{}}Função de custo e\\ Base de treinamento\end{tabular}}
				& \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}	Controle de\\ taxa e /ou qualidade \end{tabular}}                                                                                                                                    & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Comparação com\\  codecs tradicionais\end{tabular}}                                                                           \\ \hline
				Toderici et. al.  \cite{Variable2016Toderici}       & \begin{tabular}[c]{@{}l@{}}- Totalmente conectado \\ - Recorrente \\ - Convolucional \\ - Convolucional LSTM\\ - Encadeamento implícito \\ ou explícito  de ACs\end{tabular} & \begin{tabular}[c]{@{}l@{}}- Modelo opera sobre os \\ resíduos  em  16 iterações \\  - Reconstrução aditiva ou \\ única\end{tabular}                                                                                                           & \begin{tabular}[c]{@{}l@{}}- Norma $L_2$ aplicada nos resíduos\\ - Base com 216 milhões de imagens\\  obtidas da internet e subamostradas \\ para blocos $32 \times 32$\end{tabular}                                           & \begin{tabular}[c]{@{}l@{}}- Controle da taxa é\\ feito pelo número de \\ iterações na reconstrução\\ dos blocos.\end{tabular}                                                                                                                              & \begin{tabular}[c]{@{}l@{}}- O modelo convolucional\\  LSTM  supera  o JPEG \\ em SSIM  dentro do \\ intervalo 0,125 a 1,375 bpp.\end{tabular}                                                       \\ \hline
				Toderici et. al.  \cite{FullResolution2017Toderici} & \begin{tabular}[c]{@{}l@{}}- Convolucional LSTM \\ - Convolucional LSTM\\ associativa\\ - Convolucional GRU\\ - Encadeamento implícito\end{tabular}                          & \begin{tabular}[c]{@{}l@{}}- Modelo opera sobre os \\ resíduos  em  16 iterações\\ - Reconstrução aditiva ou \\ única\\ - RNA para camada de \\ fator de ganho dos resíduos\\ - RNA para gerar contexto\\ de codificação aritmética\end{tabular} & \begin{tabular}[c]{@{}l@{}}- Norma $L_1$ aplicada nos resíduos\\ - Base reunida em \cite{Variable2016Toderici} e um novo\\ conjunto de blocos $32 \times 32$ de alta\\ entropia recortados de 6 milhões\\ de imagens $1280 \times 720\textbf{}$ que foram\\ coletadas aleatoriamente da internet	
				\end{tabular}                                           & \begin{tabular}[c]{@{}l@{}}- Controle da taxa é\\ feito pelo número de \\ iterações na reconstrução\\ dos blocos\\ - Codificação de entropia\\ reduz a taxa\end{tabular}                                                                                    & \begin{tabular}[c]{@{}l@{}}- O modelo Convolucional\\  GRU  de reconstrução \\ única supera o JPEG \\ em  MS-SSIM e PSNR-HVS\\ dentro  do intervalo 0,1 a 1,5 \\  bpp (aproximadamente)\end{tabular} \\ \hline
				Covell et. al.  \cite{target}                       & \begin{tabular}[c]{@{}l@{}}- Convolucional LSTM\\ - Encadeamento implícito\end{tabular}                                                                                      & \begin{tabular}[c]{@{}l@{}}- Modelo opera sobre os \\ resíduos  em  16 iterações.\\ -  Reconstrução aditiva \\ com ganho residual\\ - Implementação de \\ códigos de parada\\  - LZ77 é usado para \\ codificação de entropia\end{tabular}     & \begin{tabular}[c]{@{}l@{}}- Norma $L_1$ aplicada nos resíduos \\ e penalidade pela ocorrência de \\bits diferentes de zero\\ - Base de treinamento formada por \\blocos $32 \times 32$\end{tabular} & \begin{tabular}[c]{@{}l@{}}- Controle da taxa é\\ feito pelo número de \\ iterações na reconstrução\\ dos blocos\\ - A qualidade alvo é \\ usada  para  adaptar  as\\  taxas  de bits por\\  blocos\\ - Codificação de entropia\\ reduz a taxa\end{tabular} & \begin{tabular}[c]{@{}l@{}}- O modelo supera o JPEG em \\ PSNR dentro  do intervalo \\ 0.125 a 1,375 bpp\\  (aproximadamente)\end{tabular}                                                           \\ \hline
				Minnen et. al.  \cite{SpatiallyAdaptive2018Minnen}  & \begin{tabular}[c]{@{}l@{}}- Convolucional LSTM\\ - Encadeamento implícito\end{tabular}                                                                                      & \begin{tabular}[c]{@{}l@{}}- Modelo opera sobre os \\ resíduos  em  16 iterações.\\ - AC para  predição de  \\ contexto\\ - Reconstrução aditiva\end{tabular}                                                                                  & \begin{tabular}[c]{@{}l@{}}- AC de contexto: norma $L_1$ \\ sobre o resíduo\\ - AC Conv2DLSTM: função de \\custo não especificada \\- Base de treinamento composta\\ por blocos $64 \times 64$ de alta entropia\\ recortados de 6 milhões de imagens\\ $1280 \times 720$,  seguindo método em \cite{FullResolution2017Toderici}   \end{tabular} & \begin{tabular}[c]{@{}l@{}}- Controle da taxa é\\ feito pelo número de \\ iterações na reconstrução\\ dos blocos\end{tabular}                                                                                                                               & \begin{tabular}[c]{@{}l@{}}- O modelo supera o JPEG\\  em PSNR  dentro do \\ inverva-lo de 0,25 a 1,5 bpp\end{tabular}                                                                               \\ \hline
				Jonhston et. al.  \cite{Priming2017Johnston}        & \begin{tabular}[c]{@{}l@{}}- Convolucional LSTM\\ - Encadeamento implícito\end{tabular}                                                                                      & \begin{tabular}[c]{@{}l@{}}- Codificação com uma\\  arquitetura de \\ 16 iterações que memória.\\  - iterações falsas\\  - Método de alocação de\\ bits pós treinamento\end{tabular}                                                           & \begin{tabular}[c]{@{}l@{}}- Norma $L_1$ sobre os resíduos  \\ponderada pela DSSIM \\
					- Base de treinamento é formada \\por blocos $128 \times 128$ amostrados \\aleatoriamente de 6 milhões de\\ imagens $1280 \times 720$ da internet
					
				\end{tabular}                        & \begin{tabular}[c]{@{}l@{}}- Controle da taxa é\\ feito pelo número de \\ iterações na reconstrução\\ dos blocos\\  - A qualidade alvo é \\ usada  para  adaptar  as\\  taxas  de bits por blocos\end{tabular}                                              & \begin{tabular}[c]{@{}l@{}}O modelo supera o BPG\\  no MS-SSIM dentro do \\ intervalo 0,125 a 2  bpp \\ (aproximadamente)\end{tabular}                                                               \\ \hline
			\end{tabular}\quad
		\end{adjustbox}
		\label{table:codecs}
	\end{table}
\end{landscape}

%\end{landscape}

\section{Autocodificadores variacionais}



Diferentemente dos autocodificadores que produzem diretamente $z$ no gargalo, os VAE produzem duas matrizes, representando média e variância, para caracterizar os gaussianos. Essa matriz representa o $P(z)$ (PDF da variável latente $z$) e é feita uma amostragem. Portanto, o codificador e o decodificador são as funções usadas para induzir a distribuição e mapear os gaussianos fatorados para a distribuição real, reconstruindo a amostra.


O autocodificador variacional foi empregado em  \cite{End2016Balle} na tarefa de compressão de imagens ao tentar modelar taxa e distorção, $R +\lambda D$. Os autores desenvolveram uma estrutura para otimização de ponta a ponta de um modelo de compressão de imagem baseado em transformações não-lineares \ref{fig:balle1}. 

Eles usaram uma módulo de não-linearidade denominado normalização divisiva generalizada (GDN) que tem se mostrado eficaz na gaussianização de densidades de imagem. 


A transformação de análise $g_a$ consiste em três estágios de convolução, subamostragem e normalização divisiva. Analogamente, a transformada de síntese $g_s$ consiste em três estágios, com a ordem das operações invertida dentro de cada estágio: IGDN (inverso aproximado da GDN), superamostragem e convolução.


Esta transformação é seguida por quantização escalar uniforme (isto é, cada elemento é arredondado para o número inteiro mais próximo), o que implementa efetivamente uma forma paramétrica de quantização vetorial no espaço da imagem original. 
A imagem comprimida é reconstruída a partir destes valores quantificados usando uma transformação inversa não linear paramétrica aproximada.

Para qualquer ponto desejado ao longo da curva de taxa-distorção, os parâmetros de ambas as transformações de análise e síntese são otimizados em conjunto usando a descida de gradiente estocástica. Para conseguir isso na presença de quantização (que produz zero gradientes em quase todos os lugares), usamos uma função de perda de proxy baseada em um relaxamento contínuo do modelo de probabilidade, substituindo a etapa de quantização por ruído uniforme aditivo. 
Aqui, definimos a transformação perceptual $g_p$ para a identidade e usamos o erro quadrático médio (MSE) como a métrica (ou seja, d (z, zˆ) = kz - zˆk22). 

A Figura \ref{fig:balle1}, extraída desse trabalho, mostra a arquitetural geral. Para esse fim, a seguinte função objetivo é usada:


Em vez de tentar uma quantização ótima diretamente no espaço da imagem, que é intratável devido à alta dimensionalidade, assumimos um quantizador escalar fixo uniforme no espaço de código, e pretendemos que as transformações não-lineares distorcem o espaço de maneira apropriada, implementando efetivamente uma forma paramétrica de quantização vetorial.

As taxas reais obtidas por um código de entropia apropriadamente projetado são apenas ligeiramente maiores que a entropia (Rissanen e Langdon, 1981), e assim definimos o objetivo funcional diretamente em termos de entropia:

\begin{equation}
L[g_a, g_s, P_q] = - \mathbb{E}[\log_2 P_q] + \lambda \mathbb{E}[d(z, \hat{z})]
\end{equation}

Portanto, o primeiro termo corresponde à entropia do código, modelado por $P_q$ e o segundo termo corresponde à perda de reconstrução. Ambas as expectativas serão aproximadas por médias ao longo de um conjunto de imagens. 

onde ambas as expectativas serão aproximadas por médias de um conjunto de imagens de treinamento. Dado um conjunto suficientemente poderoso de transformações, podemos supor sem perda de generalidade que o tamanho do escaninho de quantização é sempre um e os valores representativos estão no centro dos escaninhos. Isso é,

\begin{equation}
\hat{y}_i = q_i = round(y_i)
\end{equation}









Aqui, fazemos uso de uma transformação de normalização divisiva generalizada (GDN) com parâmetros otimizados, que anteriormente mostramos ser altamente eficientes na Gaussianização de estatísticas de junções locais de imagens naturais, muito mais do que cascatas de transformações lineares seguidas por não linearidades pontuais ( Ballé, Laparra e Simoncelli, 2015).



Note que alguns algoritmos de treinamento para redes convolucionais profundas incorporam a “normalização de lotes”, reescalonando as respostas dos filtros lineares na rede, de modo a mantê-lo em uma faixa operacional razoável (Ioffe e Szegedy, 2015). Esse tipo de normalização é diferente do controle de ganho local, pois o fator de redimensionamento é idêntico em todos os locais espaciais. Além disso, quando o treinamento é concluído, os parâmetros de escala são tipicamente fixos, o que transforma a normalização em uma transformação afim em relação aos dados - ao contrário da GDN, que é espacialmente adaptável e pode ser altamente não-linear.




Finalmente, em vez de relatar estimativas de entropia diferencial ou discreta, implementamos um código de entropia e relatamos o desempenho usando taxas de bits reais, demonstrando assim a viabilidade de nossa solução como um método completo de compactação com perdas.






Diferentemente dos autocodificadores que produzem diretamente $z$ no gargalo, os VAE produzem duas matrizes, representando média e variância, para caracterizar os gaussianos. Essa matriz representa o $P(z)$ e é feita uma amostragem. 

\begin{figure}
	\centering
	\includegraphics[width=0.80\textwidth]{figuras/balle_1.pdf}
	\caption{O vetor de imagem $x \in \mathbb{R}^N$ é mapeado para o espaço do código com $y = g_a(x,\phi)$. $y$ é quantizado, produzindo um vetor $q \in \mathbb {Z}^M $. A taxa $R$ é limitada pela entropia do setor quantizado, $H[P_q]$. Em seguida, os elementos discretos $ q $ são interpretados como $\hat{y}$ de valor contínuo, que é transformado em espaço de dados com $\hat{x} = g_s (\hat{y}, \theta)$. A comparação da reconstrução é feita em um espaço desejado mapeado por $g_p$. Os parâmetros $\phi$ e $\theta$ são otimizados para minimizar a distorção da taxa $R + \lambda D$ \cite{End2016Balle}.}
	\label{fig:balle1}
\end{figure}

Portanto, o codificador e o decodificador são as funções usadas para induzir a distribuição e mapear os gaussianos fatorados para a distribuição real, reconstruindo a amostra.

Embora os VAE sejam usados no paradigma generativo e aparentemente não tenham nada a ver com codificação, essas arquiteturas podem ser usadas para fins de codificação. Na tentativa de modelar diretamente a distorção da taxa, $R + \lambda D$, VAE são usados em \cite{End2016Balle}. A Figura \ref{fig:balle1}, extraída desse trabalho, mostra a arquitetura geral. Para esse fim, a seguinte função objetivo é usada:

\begin{equation}
L[g_a, g_s, P_q] = - \mathbb{E}[\log_2 P_q] + \lambda \mathbb{E}[d(z, \hat{z})]
\end{equation}

Assim, o primeiro termo corresponde à entropia do código, modelado por $P_q$ e o segundo termo corresponde à perda de reconstrução. Ambas as expectativas serão aproximadas por médias sobre um conjunto de imagens. A quantização tem tamanho de caixa de um:

\begin{equation}
\hat{y}_i = q_i = round(y_i)
\end{equation}

e a densidade marginal de $\hat{y}_i$ é dada por um trem de massas de probabilidade discretas com pesos iguais à função de massa de probabilidade de $q_i$:

\begin{equation}
P_{q_i} (n) = \int_{n - \frac{1}{2}}^{n + \frac{1}{2}} p_{y_i}(t) dt, \forall n \in \mathbb{Z}
\end{equation}

Um problema apontado pelos autores é que, uma vez que essa modelagem depende de valores quantizados, os derivados são zero em quase todos os lugares. Então eles substituem o quantizer com ruído uniforme i.i.d aditivo, $\tilde{y} = y + \Delta y$, que é um relaxamento contínuo do pmf de $q$:

\begin{equation}
p_{\tilde{y}} (n) = P_q(n), \forall n \in \mathbb{Z}^M
\end{equation}

Ele permite que a entropia diferencial de $\tilde{y}$ seja usada como uma aproximação da entropia de $q$. Para modelar a probabilidade de $\tilde{y}$ os autores usam funções lineares em sentido de sentido sinuoso. Assim, a perda específica a ser otimizada pode ser colocada da seguinte forma \cite{End2016Balle}:

\begin{equation}
\begin{aligned}
L(\theta, \phi) = \mathbb{E}_{x, \Delta y} [ - \sum_{i}^{} \log_2 p_{\tilde{y}_i}(g_a(x; \phi) + \Delta y; \psi^{(i)}) \\
+ \lambda d(g_p(g_s(g_a(x; \phi) + \Delta y; \theta)),g_p(x))] 
\end{aligned}
\end{equation}

Esta formulação, derivada do problema de distorção de taxa, assemelha-se ao contexto de autoencodificadores variacionais, uma vez que uma função de perda contínua é usada. Como explicado pelos autores, em inferência variacional bayesiana, dadas as observações de variáveis $x$ juntamente com $p_{x y}(x||y)$ um posterior $p_{y|x}(y|x)$ é perseguido, que não pode ser expresso na forma fechada. Usando a divergência Kullback-Leibler

\begin{equation}
\begin{aligned}
D_{KL} [q||q_{y|x}] &= \mathbb{E}_{y \sim q} \log q(y|x) - \mathbb{E}_{y \sim q} \log p_{y|x}(y|x) \\
&= \mathbb{E}_{y \sim q} \log q(y|x) - \mathbb{E}_{y \sim q} \log p_{x|y}(x|y) -  \mathbb{E}_{y \sim q} \log p_y(y) + const.
\end{aligned}
\end{equation}
Os autores mostram que essa função objetiva é equivalente ao problema de otimização de distorção de taxa relaxado, com distorção medida como MSE, definindo o modelo da seguinte forma:

\begin{equation}
p_{x|\tilde{y}}(x|\tilde{y};\lambda,\theta) = \mathcal{N}(x; g_s(\tilde{y}; \theta), (2\lambda)^{-1} 1)
\end{equation}
\begin{equation}
p_{\tilde{y}}(\tilde{y}; \psi^{(0)}, \psi^{(1)},...) = \prod_{i}^{} p_{\tilde{y}_i} (\tilde{y}_i; \psi^{(i)})
\end{equation}

e o posterior como:
\begin{equation}
q(\tilde{y}|x; \phi) = \prod_{i}^{} \mathcal{U}(\tilde{y}_i; y_i; 1) \text{ with } y = g_a(x; \phi)
\end{equation}

onde $\mathcal{U}(\tilde{y}_i; y_i; 1)$ é a densidade uniforme no intervalo da unidade centrada em $y_i$. Com esta formulação, o primeiro termo em KL-divergência é constante, o segundo é a distorção eo terceiro é a taxa.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/balle_2.pdf}
	\caption{The description of the model considering the hyperprior \cite{Variational2018Balle}.}
	\label{fig:balle2}
\end{figure}

Para a configuração de treinamento, foram utilizadas 6507 imagens do ImageNet. Essa otimização foi realizada para cada $\lambda$ e o codificador de entropia foi uma implementação inspirada no CABAC. A avaliação foi realizada no conjunto de dados da Kodak. Muitos outros trabalhos são baseados na formulação apresentada neste trabalho. Por exemplo, em \cite{Variational2018Balle} o modelo acima é melhorado usando hiperpriors. A Figura ref{fig:balle2}, apresentada nesse trabalho, mostra o novo modelo utilizado.

Neste trabalho, cada $\tilde{y}_i$ é modelado como um Gaussian zero-médio com um desvio padrão $\sigma_i$, obtido por um VAE auxiliar. Então:

\begin{equation}
p_{\tilde{y}|\tilde{z}} (\tilde{y}|\tilde{z}) = \prod_{i}^{} (\mathcal{N}(0, \tilde{\sigma}_i^2) * \mathcal{U}(-\frac{1}{2}, \frac{1}{2}))(\tilde{y}_i)
\end{equation}
com $\tilde{\sigma} = h_s (\tilde{z}; \theta_h)$. Este modelo é ampliado ainda mais por empilhamento $h_a $ em cima de $y $ criando um posterior variacional fatorado:
\begin{equation}
q(\tilde{y},\tilde{z}|x, \phi_g, \phi_h) = \prod_{i}^{} \mathcal{U}(\tilde{y}_i | y_i - \frac{1}{2}, y_i + \frac{1}{2}) . \prod_{j}^{} \mathcal{U}(\tilde{z}_j | z_i - \frac{1}{2}, z_i + \frac{1}{2})
\end{equation}

com $y = g_a (x; phi_g)$ e $z = h_a (y; phi_h)$. A modelagem de $z$, o hiperprior, usa o modelo totalmente fatorado usado no trabalho anterior por $\tilde{y}$:

\begin{equation}
p_{\tilde{z} | \psi}(\tilde{z} | \psi) = \prod_{i}^{} (p_{\tilde{z}_i | \psi^{(i)}}(\psi^{(i)}) * \mathcal{U}(-\frac{1}{2}, \frac{1}{2}))(\tilde{z}_i)
\end{equation}


Considerando esta formulação, a perda é definida por \cite{Variational2018Balle} como:


\begin{equation}
\begin{aligned}
\mathbb{E}_{x \sim p_x} D_{KL}[q || p_{\tilde{y}, \tilde{z}|x}] = \mathbb{E}_{x \sim p_x} \mathbb{E}_{\tilde{y}, \tilde{z} \sim q}[\log q(\tilde{y}, \tilde{z}|x) - \log p_{x|\tilde{y}}(x|\tilde{y}) \\
- \log p_{\tilde{y} | \tilde{z}}(\tilde{y} | \tilde{z}) - \log p_{\tilde{z}}(\tilde{z})] + const.
\end{aligned}
\end{equation}

onde, novamente, o primeiro termo é zero, uma vez que $q$ é um produto de densidades uniformes de largura da unidade. O segundo termo é a distorção, enquanto o terceiro e quarto mandato representam as entropias cruzadas de $\tilde{y}$ e $\tilde{z}$. O quarto termo pode ser visto como informação lateral.

A configuração de treinamento deste novo trabalho compreende um conjunto de 1 milhão de imagens JPEG da web, com tamanho 3000 x 5000. Estas imagens foram amostradas para 640 x 1200 pixels e 256 x 256 culturas das imagens foram extraídas. O lote é composto por 8 amostras e a taxa de aprendizagem é de $10^{-4}$. Em \cite{Variational2018Balle} os autores mencionam que a normalização em lotes e alguns horários de taxa de aprendizagem não tiveram efeitos benéficos neste caso. Um total de 16 modelos com hiperpriors foram treinados.

Vale a pena mencionar, como com o trabalho anterior, que uma desvantagem dessa abordagem é a necessidade de treinar muitos modelos para diferentes distorções de taxa. Um problema não presente nas abordagens descritas nas sessões anteriores. A configuração do teste é, novamente, o conjunto de dados kodak, onde melhores resultados foram obtidos neste trabalho do que no anterior. Uma tabela que descreve estes resultados será exibida em uma sessão mais atrasada.

\begin{figure}
	\centering
	\includegraphics[width=0.90\textwidth]{figuras/balle_3.pdf}
	\caption{The context model is the autoregressive component which predicts latents from context. There's also the hyperprior, seen in earlier proposal of this architecture. Both models feed a network of entropy parameters which outputs information about the distribution of the code model to the basic VAE \cite{Autoregressive2018Minnen}.}
	\label{fig:figuras/balle3}
\end{figure}

Outra melhoria no modelo apresentado em \cite{Variational2018Balle} é vista em \cite{Autoregressive2018Minnen}. Este trabalho traz um modelo de estrutura mais complexo usando um modelo autoregressivo através de redes neurais e, novamente, o hiperprior. Esses modelos são combinados para alcançar uma modelagem mais robusta do modelo de código. A Figura ref{fig:balle3}, extraída deste trabalho, mostra toda a arquitetura.

Nesta proposta, o modelo para latente $\hat{y}_i$ é novamente definido como um gaussian envolvido com uma distribuição uniforme da unidade. Mas desta vez, o modelo é expandido considerando que os componentes de média e escala dos gaussians são previstos pelo modelo:

\begin{equation}
\begin{aligned}
p_{\hat{y}}(\hat{y}|\hat{z}, \theta_{hd}, \theta_{cm}, \theta_{ep}) &= \prod_{i}^{} (\mathcal{N}(\mu_i, \sigma_i^2) * \mathcal{U}(-\frac{1}{2}, \frac{1}{2}))(\hat{y}_i) \\
\mu_i, \sigma_i^2 &= g_{ep}(\psi, \phi_i; \theta_{ep}) \\
\psi &= g_h (\hat{z};\theta_{hd}) \\
\phi_i &= g_{cm}(\hat{y}_{<i}; \theta_{cm})
\end{aligned} 
\end{equation}

onde $g_{ep}$ é a rede de parâmetros de entropia, $g_h$ é a rede hiperprior, $\phi_i$ é a rede de modelos de contexto e $\hat{y}_{<i}$ é o contexto causal de cada latente $\hat{y}_i$. As versões chapéu aqui correspondem à quantização das variáveis, que, novamente, é relaxado no tempo de treinamento usando $\mathcal{U}$, o ruído uniforme aditivo. Como não há suposições sobre a distribuição dos hiper-latents, um modelo não paramétrico e totalmente fatorado é usado. A função de perda, neste caso, é semelhante à versão do trabalho anterior:

\begin{equation}
R + \lambda D = \mathbb{E}_{x \sim p_x}[-\log_2 p_{\hat{y}}(\hat{y})] + \mathbb{E}_{x \sim p_x}[-\log_2 p_{\hat{z}}(\hat{z})] + \lambda  \mathbb{E}_{x \sim p_x} ||x - \hat{x}||_2^2
\end{equation}

onde o primeiro termo é a taxa dos latents, o segundo termo é a taxa dos hiper-latents e $\lambda$-term é a distorção. Este trabalho foi o primeiro a superar o BPG em métricas de distorção PSNR e MS-SSIM.

Algumas abordagens apresentadas na literatura são baseadas nessas obras apresentadas anteriormente. Por exemplo, \cite{Lossy2017Theis} propõe uma estrutura de compressão de imagem com base em \cite{End2016Balle}. Os autores se concentram na eficiência computacional, uma vez que os trabalhos baseados em VAE exigem treinamento de muitos modelos, para cada $\lambda$, que é computacionalmente caro. Uma primeira diferença na arquitetura diz respeito à quantização. Diferente de \cite{End2016Balle}, os autores não substituem totalmente a quantização por ruído uniforme. Em vez disso, apenas os gradientes são substituídos enquanto a quantização é realizada no tempo de treinamento.

Conforme formulado pelos autores, em vez dos gradientes mal definidos da quantização, o derivado é o de uma aproximação suave, $r$, que depende de y:

\begin{equation}
\frac{d}{dy}[y] = \frac{d}{dy} r(y)
\end{equation}


Os autores afirmam que não há uma substituição totalmente com a aproximação suave, porque o decodificador pode aprender a inverter a aproximação suave. O resultado é que ele pode remover o gargalo de informações que força a rede a comprimir informações. Empiricamente, eles usam a identidade $r (y) = y$, que funciona bem como escolhas sofisticadas. Desta forma, os gradientes podem ser propagados sem modificações.

Uma vez que a arquitetura é baseada em VAE, há também o problema de usar um modelo de entropia discreto $Q $ que não tem gradientes. Assim, uma aproximação contínua e diferenciada $q $ é usada, definindo um limite superior à entropia de $Q$:
\begin{equation}
\begin{aligned}
Q(z) &= \int_{[-.5,.5[^M}^{} q(z + u) du \\
-\log_2 Q(z) &= -\log_2 \int_{[-.5,.5[^M}^{} q(z + u) du \leq \int_{[-.5,.5[^M}^{} -\log_2 q(z+u) du
\end{aligned}
\end{equation}

com o segundo passo após a desigualdade de Jensen. A principal diferença é proposta para as taxas de bitvariável. A maneira padrão, até agora, é treinar modelos diferentes para curvas diferentes da taxa-distorção. Para reduzir os requisitos computacionais, os autores aperfeiçoam um autocodificador pré-treinado atraindo um $\lambda \in mathbb{R}^M$

\begin{equation}
-\log_2 q([f(x) \odot \lambda] + u) + \beta d(x, g(\frac{[f(x) \odot \lambda]}{\lambda}))
\end{equation}

onde $\odot$ representa multiplicação pontiaguda. E para reduzir o número de escalas treináveis, elas também são compartilhadas entre as dimensões. Com este $\lambda$ é possível controlar a quantidade de informação rejeitada na quantização. Por exemplo, se o valor $0.3583$ aparecer em um latente, a operação redonda renderia $0$. Em vez disso, se for multiplicado por US $10$, a operação redonda devolveria US $3$. Considerando uma escala de $100$, o valor arredondado seria $35$. No decodificador, uma vez que a operação de escala é reversível, há mais informações que passam do codificador para o decodificador.

A ideia de usar esses parâmetros de escala é implementada da seguinte forma: o autocodificador é treinado otimizando um $\beta$. Depois disso, este autocodificador é ajustado novamente com um pequeno número de iterações para cada um dos $\lambda$'s. Além disso, para aumentar a densidade do $\lambda$'s, uma interpolação é realizada nesses pontos.

Para modelar as distribuições dos coeficientes e estimar a taxa de bits, são utilizadas misturas de escala gaussiana:
\begin{equation}
\log_2 q(z+u) = \sum_{i,j,k}^{} \log_2 \sum_{s}^{} \pi_{ks} \mathcal{N}(z_{kij} + u_{kij}; 0, \sigma^2_{ks})
\end{equation}

Adam é o otimizador aplicado em lotes de 32 imagens de $128 vezes 128$ pixels. Esse treinamento é realizado usando uma estratégia diferente, que os autores chamam de treinamento "incremental". Isso é feito introduzindo uma máscara binária $m$:
\begin{equation}
-\log_2 q([f(x)] \odot m + u) + \beta d(x, g([f(x)] \odot m))
\end{equation}
Esta máscara começa com apenas duas entradas não-zero e esse valor é aumentado cada vez que um limite de desempenho é atingido pela rede. Depois que a máscara é totalmente habilitada, a taxa de aprendizagem é reduzida de $10^{-4}$ para $10^{-5}$. O treinamento foi realizado com  $10^6$ iterações. A configuração de treinamento do $\lambda$ fine-tune foi uma taxa de aprendizagem inicial de $10^{-3}$ aumentada por um fator de $\frac{.8^\kappa}{{(1000 + t)}^\kappa}$.  

As escalas foram otimizadas, como explicado anteriormente, para 10000 iterações. Os resultados alcançados superam o JPEG2000 em termos de SSIM.

%O objetivo de tal rede é otimizar o trade-off entre a quantidade de distorção e a eficiência da compressão, geralmente expressa pela taxa de bits ou bits por pixel (bpp).

%Para o estimador de entropia H, além do modelo GSM mencionado na seção anterior, trabalhos recentes também utilizam modelos gerativos[5] e modelos de contexto[8] como alternativas potenciais. Para Trabalhos de abordagens não autoencoders, há também um interesse crescente no uso de GAN [9], GDN (Generalized Divisive Normalization) [5] e RNN [10]

\section{Autocodificadores baseado em GAN}


No artigo \cite{akbari2019dsslic} é proposto uma estrutura de aprendizagem profunda de compressão de imagem aliada a segmentação semântica e um modelo de geração de dados. 
Nessa abordagem, o codificador gera 3 tipos de informação que irão compor o fluxo de bits. A primeira se refere ao mapa de segmentação semântica da imagem de entrada. A segunda diz respeito a uma representação decimada da imagem de entrada. Ainda no codificador, o mapa de segmentação e a versão compacta da imagem são entradas do gerador de uma GAN que sintetiza uma reconstrução grosseira da imagem. O residual entre a entrada e a reconstrução grosseira é a terceira camada de informação codificada. Arquitetura é composta de 3 redes: \textit{SegNet}, \textit{CompNet} e \textit{FineNet} (gerador). O uso de GANS é com múltiplas escalas de discriminadores. 
A função de custo é formada por medidas de perdas em pixels, perdas perceptivas e treinamento antagônico.

%\cite{akbari2019dsslic}.

Resultados experimentais mostram que a estrutura proposta supera o BPG baseado em H.265/HEVC e outros codecs nas métricas PSNR e MS-SSIM em uma ampla faixa de taxas de bits. Além disso, como o mapa de segmentação semântica está incluído no fluxo de bits, o esquema proposto pode facilitar muitas outras tarefas, como pesquisa de imagens e compactação de imagem adaptativa baseada em objetos \cite{akbari2019dsslic}.


Em \cite{zhao1901cae}, os autores adotaram um autocodificador compressivo em que a base do codificador são camadas convolucionais e o decodificador espelha a estrutura do codificador, exceto que usa  camadas convolucionais \textit{sub-pixel} \cite{li2018learning}. O problema de otimização que a rede tenta alcançar é, a já conhecida, distância $d$ entre as imagens original e reconstruída e o número de bits necessários $R$ para armazenar o código latente $z$.
\begin{equation}
\begin{aligned}
L = d(x,x') + \lambda R(z) 
\end{aligned}
\end{equation}

Para otimizar $R$ ao invés de usar um estimador de entropia $H$ é empregado uma medida que penaliza o sinal de erro quando o número de elementos diferente de zero no latente for superior a um número \textit{l} desejado. Esse problema foi resolvido aplicando o algoritmo ADMM (Método de Direção Alternativa dos Multiplicadores) \cite{ye2018progressive}. 



%Para aprender um modelo DNN compressível ou representação de recurso, precisamos minimizar D + βR, onde β> 0 controla o trade-off de distorção de taxa. A inclusão da entropia na função de custo de aprendizado pode ser vista como a adição de um regularizador que promove uma representação compressível da rede ou da representação de recursos. No entanto, dois grandes desafios surgem ao minimizar o D + βR para DNNs: i) lidar com a não diferenciabilidade (devido a operações de quantização) da função de custo D + βR e ii) obter uma estimativa precisa e diferenciável da entropia (isto é, , R). Para enfrentar i), vários métodos foram propostos. Entre as mais populares estão as aproximações estocásticas [39, 19, 7, 32, 5] e o arredondamento com uma aproximação derivada suave [15, 30]. Para abordar ii) uma abordagem comum é assumir que o fluxo de símbolos seja i.i.d. e modelar a distribuição marginal de símbolos com um modelo paramétrico, como um modelo de mistura gaussiana [30, 34], um modelo linear por partes [5] ou uma distribuição de Bernoulli [33] (no caso de símbolos binários).

%Todos os algoritmos de compressão envolvem um par de transformações de análise e síntese que visam reproduzir com precisão as imagens originais. Os codecs artesanais tradicionais carecem de adaptabilidade e são incapazes de aproveite a redundância semântica em imagens naturais. 



No artigo \cite{santurkar2018generative} propõe-se uma estrutura unificada de aprendizado de ponta a ponta para aprender representações compressíveis, otimizando conjuntamente os parâmetros do modelo, os níveis de quantização e a entropia do fluxo de símbolos resultante. O objetivo é na compressão de imagens em miniatura.
Adota-se um paradigma da compressão generativa, onde treina-se primeiro a transformação de síntese como modelo generativo. Os autores utilizaram a rede DCGAN \cite{radford2015unsupervised} para esse fim. Essa transformação de síntese é então usada como um decodificador não adaptável como parte da configuração de um autocodificador. Assim, limita-se o espaço de pesquisa das reconstruções a um conjunto compacto menor de imagens naturais que leva em consideração a redundância semântica \cite{santurkar2018generative}. 
Nesse trabalho, os resultados não supera os codecs padrão, no entanto, é consideravelmente mais resiliente às taxas de erros de bits (por exemplo, de canais sem fio ruidosos) do que os esquemas de codificação de entropia de comprimento variável tradicionais.



Em \cite{GAN2017Rippel}, Rippel et al. proporam um modelo de treinamento adversarial em várias escalas para incentivar as reconstruções nítidas e próximas à imagem original, mesmo com taxas de bits muito baixas. Foi o primeiro trabalho trabalho a trabalhar com GANs para compactação de imagem. 

De forma resumida, a primeira operação do modelo é extrair os recursos da imagem através de redes convolucionais. Os recursos correspondem a um vetor compactado com as características úteis da imagem original. Para isso, a partir da imagem original são obtidos novas imagens em escalas diferentes. Dessas imagens são obtidos os recursos formando uma  "decomposição piramidal''. Em seguida há um procedimento de alinhamento que explora a estrutura compartilhada de tais recursos.  

O segundo módulo é responsável por compactar ainda mais os recursos extraídos. Ele quantiza os recursos e os codifica por meio de um esquema de codificação aritmética adaptativa aplicado em suas expansões binárias. Uma regularização adaptativa do comprimento do código é introduzida para penalizar a entropia dos recursos, que o esquema de codificação explora para obter uma melhor compactação. 
A função custo é composta pela distorção entre o alvo e sua reconstrução e a perda do discriminador devido ao treinamento antagônico com GANs para buscar reconstruções realistas.

Agustsson et. al. no artigo \cite{agustsson2019generative}, propuseram uma estrutura baseada em GAN para compactação generativa aprendida, além disso foi apresentado o primeiro estudo completo dessa estrutura para compactação de imagem em resolução total. 

Para comprimir uma imagem $x$, seguiram a formulação de autocodificador composto por um codificador $E$, decodificador/gerador $G$  e um quantizador finito $q$. 
Aqui, o codificador $E$ mapeia a imagem para um mapa de características latentes $z$, cujos valores são quantizados para $L$ níveis [$C = {c_1,. . . , c_L}  \in \mathbb{R}$]  para obter uma representação $\hat{z} = q(E(x))$.
O decodificador é representado pela rede geradora de uma GAN condicional \cite{mirza2014conditional}  que tenta recuperar a imagem formando uma reconstrução $\hat{x}$.

Para poder retropropagar o sinal de erro através da quantização é usado um relaxamento diferenciável para $q$. Nessa formulação, controla-se a entropia, e portanto a taxa, pela escolha do número de níveis de quantização, uma vez que a entropia obedece  $H(\hat{z}) \leq dim(\hat{z}) \log_2(L)$  \cite{agustsson2019generative}. 

A função de perdas usada como sinal para atualizar os pesos é composta por um termo de distorção da informação reconstruída e a perda do discriminador D da GAN, $\mathcal{L}_{gan}$. Em relação à interação entre a perda GAN e a perda MSE, observa-se que a perda MSE estabiliza o treinamento, pois penaliza o colapso do GAN  \cite{agustsson2019generative}.

Nesse trabalho, os resultados mostram que, para baixas taxas de bits, essa compactação generativa pode proporcionar uma economia drástica de taxa de bits em comparação com os métodos de ponta anteriores otimizados para objetivos clássicos, como MS-SSIM e MSE, quando avaliados em termos de qualidade visual em um estudo de usuário. 
