\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\gdef\pagesLTS@loaded{p@gesLTSnotlo@ded}
\citation{abnt-url-package=url}
\catcode `"\active 
\newlabel{pagesLTS.0}{{}{i}{}{page.i}{}}
\newlabel{pagesLTS.0.local}{{}{1}{}{page.i}{}}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\@newglossary{symbolslist}{slg}{syi}{syg}
\@newglossary{notation}{nlg}{not}{ntn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\reset@newl@bel
\providecommand\tcolorbox@label[2]{}
\providecommand \oddpage@label [2]{}
\select@language{portuguese}
\@writefile{toc}{\select@language{portuguese}}
\@writefile{lof}{\select@language{portuguese}}
\@writefile{lot}{\select@language{portuguese}}
\select@language{portuguese}
\@writefile{toc}{\select@language{portuguese}}
\@writefile{lof}{\select@language{portuguese}}
\@writefile{lot}{\select@language{portuguese}}
\select@language{portuguese}
\@writefile{toc}{\select@language{portuguese}}
\@writefile{lof}{\select@language{portuguese}}
\@writefile{lot}{\select@language{portuguese}}
\citation{2019:ciscoreport}
\citation{h264:book}
\citation{Phyu_surveyof}
\citation{h264:book}
\select@language{portuguese}
\@writefile{toc}{\select@language{portuguese}}
\@writefile{lof}{\select@language{portuguese}}
\@writefile{lot}{\select@language{portuguese}}
\select@language{portuguese}
\@writefile{toc}{\select@language{portuguese}}
\@writefile{lof}{\select@language{portuguese}}
\@writefile{lot}{\select@language{portuguese}}
\citation{2019:ciscoreport}
\citation{h264:book}
\citation{Phyu_surveyof}
\citation{h264:book}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{portuguese}
\@writefile{toc}{\select@language{portuguese}}
\@writefile{lof}{\select@language{portuguese}}
\@writefile{lot}{\select@language{portuguese}}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{FullResolution2017Toderici}
\citation{End2016Balle}
\citation{Variational2018Balle}
\citation{Autoregressive2018Minnen}
\gdef \LT@i {\LT@entry 
    {1}{87.5276pt}\LT@entry 
    {1}{324.00119pt}}
\newlabel{pagesLTS.roman.1}{{}{xv}{}{page.xv}{}}
\newlabel{pagesLTS.roman.1.local}{{}{18}{}{page.xv}{}}
\newlabel{pagesLTS.roman}{{}{xv}{}{page.xv}{}}
\newlabel{pagesLTS.roman.local}{{}{18}{}{page.xv}{}}
\pagesLTS@ifcounter{pagesLTS.roman.1.local.cnt}
\setcounter{pagesLTS.roman.1.local.cnt}{18}
\citation{shalev2014understanding}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\citation{Haykin}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Fundamenta\IeC {\c c}\IeC {\~a}o Te\IeC {\'o}rica}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Redes Neurais Artificiais}{1}{section.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Paradigmas de programa\IeC {\c c}\IeC {\~a}o}}{1}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:machinelearning}{{1.1}{1}{Paradigmas de programação}{figure.caption.6}{}}
\citation{lecun2015deep}
\citation{bengio2009learning}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\citation{Haykin}
\citation{Haykin}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Principais componentes em uma \acrshort {rna}}}{2}{figure.caption.7}}
\newlabel{fig:modeloRNA}{{1.2}{2}{Principais componentes em uma \acrshort {rna}}{figure.caption.7}{}}
\citation{Haykin}
\citation{Haykin}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Neur\IeC {\^o}nio Artificial}{3}{subsection.1.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Modelo do neur\IeC {\^o}nio matem\IeC {\'a}tico.}}{3}{figure.caption.8}}
\newlabel{fig:neuronio}{{1.3}{3}{Modelo do neurônio matemático}{figure.caption.8}{}}
\citation{Mitchell}
\citation{mcculloch1943logical}
\citation{Haykin}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\citation{nair2010rectified}
\citation{xu2015empirical}
\citation{Mitchell}
\citation{rumelhart1988learning}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\citation{Mitchell}
\citation{Mitchell}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.1}Fun\IeC {\c c}\IeC {\~o}es de ativa\IeC {\c c}\IeC {\~a}o}{4}{subsubsection.1.1.1.1}}
\citation{Mitchell}
\citation{Mitchell}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Fun\IeC {\c c}\IeC {\~o}es de ativa\IeC {\c c}\IeC {\~o}es usadas em redes neurais artificiais e suas respectivas derivadas. Todas s\IeC {\~a}o fun\IeC {\c c}\IeC {\~o}es de uma vari\IeC {\'a}vel, exceto a fun\IeC {\c c}\IeC {\~a}o \textit  {softmax} que atua em um conjunto de entrada e retorna um grupo de sa\IeC {\'\i }da.\relax }}{5}{table.caption.9}}
\newlabel{table:func_ativacoes}{{1.1}{5}{Funções de ativações usadas em redes neurais artificiais e suas respectivas derivadas. Todas são funções de uma variável, exceto a função \textit {softmax} que atua em um conjunto de entrada e retorna um grupo de saída.\relax }{table.caption.9}{}}
\newlabel{eq:erro_neuronio}{{1.3}{5}{Funções de ativação}{equation.1.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Superf\IeC {\'\i }cie de erro para o aprendizado dos pesos de um neur\IeC {\^o}nio simples.}}{6}{figure.caption.10}}
\newlabel{fig:gradiente}{{1.4}{6}{Superfície de erro para o aprendizado dos pesos de um neurônio simples}{figure.caption.10}{}}
\newlabel{eq:desc_grad}{{1.6}{6}{Funções de ativação}{equation.1.1.6}{}}
\citation{Haykin}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\newlabel{eq:descida_grad_neuronio}{{1.8}{7}{Funções de ativação}{equation.1.1.8}{}}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\citation{hornik1989multilayer}
\citation{Goodfellow2016}
\newlabel{eq:descida_grad__estocastica_neuronio}{{1.9}{8}{Funções de ativação}{equation.1.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Redes \textit  {Feedforward}}{8}{subsection.1.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2.1}Perceptron Multicamadas}{8}{subsubsection.1.1.2.1}}
\citation{Haykin}
\citation{leshno1993multilayer}
\citation{Goodfellow2016}
\citation{Haykin}
\citation{Goodfellow2016}
\citation{Haykin}
\citation{Goodfellow2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Rede \textit  {feedforward} de 2 camadas}}{9}{figure.caption.11}}
\newlabel{fig:feedforward}{{1.5}{9}{Rede \textit {feedforward} de 2 camadas}{figure.caption.11}{}}
\newlabel{eq:teorema_mlp}{{1.10}{9}{Perceptron Multicamadas}{equation.1.1.10}{}}
\citation{Goodfellow2016}
\citation{Haykin}
\citation{geron2017hands}
\citation{FrancoisDeepLearning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2.2}Rede Convolucional}{10}{subsubsection.1.1.2.2}}
\citation{sumit}
\citation{sumit}
\citation{sumit}
\citation{sumit}
\citation{Haykin}
\citation{Haykin}
\citation{rumelhart1986learning}
\citation{FrancoisDeepLearning}
\citation{ketkar2017deep}
\citation{Goodfellow2016}
\newlabel{eq:conv2d}{{1.12}{11}{Rede Convolucional}{equation.1.1.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Convolu\IeC {\c c}\IeC {\~a}o 2D em um mapa de recurso.}}{11}{figure.caption.12}}
\newlabel{fig:con2d}{{1.6}{11}{Convolução 2D em um mapa de recurso}{figure.caption.12}{}}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Convolu\IeC {\c c}\IeC {\~a}o 2D em tr\IeC {\^e}s mapa de recurso.}}{12}{figure.caption.13}}
\newlabel{fig:con3d}{{1.7}{12}{Convolução 2D em três mapa de recurso}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Redes Recorrentes}{12}{subsection.1.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Arquitetura RNR.}}{12}{figure.caption.14}}
\newlabel{fig:rnr}{{1.8}{12}{Arquitetura RNR}{figure.caption.14}{}}
\citation{Goodfellow2016}
\citation{geron2017hands}
\citation{lstm}
\citation{geron2017hands}
\newlabel{eq:memoria}{{1.13}{13}{Redes Recorrentes}{equation.1.1.13}{}}
\newlabel{eq:lstm}{{1.14}{13}{Redes Recorrentes}{equation.1.1.14}{}}
\citation{Olah}
\citation{Olah}
\citation{xingjian2015convolutional}
\citation{FullResolution2017Toderici}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces C\IeC {\'e}lula \acrshort {lstm} no tempo}}{14}{figure.caption.15}}
\newlabel{fig:lstm_time}{{1.9}{14}{Célula \acrshort {lstm} no tempo}{figure.caption.15}{}}
\citation{FrancoisDeepLearning}
\citation{FrancoisDeepLearning}
\citation{Goodfellow2016}
\citation{Goodfellow2016}
\newlabel{eq:conlstm}{{1.15}{15}{Redes Recorrentes}{equation.1.1.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Autocodificadores}{15}{subsection.1.1.4}}
\citation{Goodfellow2016}
\citation{ng2011sparse}
\citation{kullback1951information}
\citation{ng2011sparse}
\citation{kullback1951information}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4.1}Autocodificador Esparso}{16}{subsubsection.1.1.4.1}}
\newlabel{eq:loss_sparse}{{1.18}{16}{Autocodificador Esparso}{equation.1.1.18}{}}
\citation{FrancoisDeepLearning}
\citation{kingma2013auto}
\citation{rezende2014stochastic}
\citation{klys2018learning}
\citation{FrancoisDeepLearning}
\citation{rolinek2019variational}
\citation{kingma2013auto}
\newlabel{eq:loss_sparse2}{{1.22}{17}{Autocodificador Esparso}{equation.1.1.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4.2}Autocodificador Variacional}{17}{subsubsection.1.1.4.2}}
\newlabel{elbo}{{1.24}{17}{Autocodificador Variacional}{equation.1.1.24}{}}
\citation{sonderby2016ladder}
\citation{Goodfellow2016}
\citation{gonzalez2009processamento}
\citation{sayood2017introduction}
\citation{sayood2017introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Compress\IeC {\~a}o de dados}{18}{section.1.2}}
\newlabel{eq:compressao}{{1.26}{18}{Compressão de dados}{equation.1.2.26}{}}
\citation{sayood2017introduction}
\citation{marques1999processamento}
\citation{sayood2017introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Compress\IeC {\~a}o e reconstru\IeC {\c c}\IeC {\~a}o sem perdas.}}{19}{figure.caption.16}}
\newlabel{fig:codec}{{1.10}{19}{Compressão e reconstrução sem perdas}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Medidas de informa\IeC {\c c}\IeC {\~a}o}{19}{subsection.1.2.1}}
\citation{sayood2017introduction}
\citation{gonzalez2009processamento}
\citation{richardson2010h}
\citation{marques1999processamento}
\citation{richardson2010h}
\citation{gonzalez2009processamento}
\citation{richardson2010h}
\citation{gonzalez2009processamento}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Imagens}{20}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Espa\IeC {\c c}o de Cores}{20}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.1}RGB}{20}{subsubsection.1.3.1.1}}
\citation{gonzalez2009processamento}
\citation{gonzalez2009processamento}
\citation{richardson2010h}
\citation{gonzalez2009processamento}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Espa\IeC {\c c}o tridimensional de cores RGB.}}{21}{figure.caption.17}}
\newlabel{fig:rgb}{{1.11}{21}{Espaço tridimensional de cores RGB}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.2}YCbCr}{21}{subsubsection.1.3.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Compress\IeC {\~a}o de Imagens}{21}{subsection.1.3.2}}
\citation{sayood2017introduction}
\citation{sayood2017introduction}
\citation{salomon2007data}
\citation{salomon2007data}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2.1}JPEG}{22}{subsubsection.1.3.2.1}}
\citation{sayood2017introduction}
\citation{salomon2007data}
\citation{sayood2017introduction}
\citation{richardson2010h}
\citation{richardson2010h}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Padr\IeC {\~a}o \acrshort {jpeg}}}{23}{figure.caption.18}}
\newlabel{fig:jpeg}{{1.12}{23}{Padrão \acrshort {jpeg}}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}M\IeC {\'e}tricas de Qualidade}{23}{subsection.1.3.3}}
\citation{hore2010image}
\citation{hore2010image}
\citation{wang2003multiscale}
\citation{hore2010image}
\citation{hore2010image}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.1}PSNR}{24}{subsubsection.1.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.2}SSIM}{24}{subsubsection.1.3.3.2}}
\newlabel{eq:ssim2}{{1.37}{24}{SSIM}{equation.1.3.37}{}}
\citation{wang2003multiscale}
\citation{wang2003multiscale}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.3}MS-SSIM}{25}{subsubsection.1.3.3.3}}
\citation{gersho2012vector}
\citation{End2016Balle}
\citation{santurkar2018generative}
\citation{Jiang1999}
\citation{Autoregressive2018Minnen}
\citation{akbari2019dsslic}
\citation{Variable2016Toderici}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Revis\IeC {\~a}o Bibliogr\IeC {\'a}fica}{26}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Autocodifcadores em cadeia}{27}{section.2.1}}
\citation{Variable2016Toderici}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Binariza\IeC {\c c}\IeC {\~a}o}{28}{subsection.2.1.1}}
\newlabel{subsec:bin}{{2.1.1}{28}{Binarização}{subsection.2.1.1}{}}
\newlabel{eq:quant2}{{2.3}{28}{Binarização}{equation.2.1.3}{}}
\newlabel{eq:quant}{{2.4}{28}{Binarização}{equation.2.1.4}{}}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Redes n\IeC {\~a}o-recorrentes}{29}{subsection.2.1.2}}
\newlabel{eq:reconst_escalar}{{2.8}{29}{Redes não-recorrentes}{equation.2.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Um autoencoder residual totalmente conectado com fun\IeC {\c c}\IeC {\~a}o de ativa\IeC {\c c}\IeC {\~a}o $\qopname  \relax o{tanh}$. Esta figura representa uma arquitetura de duas itera\IeC {\c c}\IeC {\~o}es. As primeiras itera\IeC {\c c}\IeC {\~o}es codificam a imagem original. Os res\IeC {\'\i }duos da reconstru\IeC {\c c}\IeC {\~a}o s\IeC {\~a}o passados para a segunda itera\IeC {\c c}\IeC {\~a}o. Cada n\IeC {\'\i }vel de itera\IeC {\c c}\IeC {\~a}o produz 4 bits \cite  {Variable2016Toderici}.\relax }}{29}{figure.caption.19}}
\newlabel{fig:toderici1}{{2.1}{29}{Um autoencoder residual totalmente conectado com função de ativação $\tanh $. Esta figura representa uma arquitetura de duas iterações. As primeiras iterações codificam a imagem original. Os resíduos da reconstrução são passados para a segunda iteração. Cada nível de iteração produz 4 bits \cite {Variable2016Toderici}.\relax }{figure.caption.19}{}}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{Variable2016Toderici}
\citation{nie2010efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces O codificador residual convolucional/deconvolucional. As camadas convolucionais s\IeC {\~a}o representadas como ret\IeC {\^a}ngulos pontiagudos, enquanto as camadas deconvolucionais s\IeC {\~a}o representadas como ret\IeC {\^a}ngulos arredondados. \cite  {Variable2016Toderici}.\relax }}{30}{figure.caption.20}}
\newlabel{fig:toderici2}{{2.2}{30}{O codificador residual convolucional/deconvolucional. As camadas convolucionais são representadas como retângulos pontiagudos, enquanto as camadas deconvolucionais são representadas como retângulos arredondados. \cite {Variable2016Toderici}.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Redes Recorrentes}{30}{section.2.2}}
\citation{FullResolution2017Toderici}
\citation{Variable2016Toderici}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces O codificador residual LSTM totalmente conectado. Esta figura mostra um desenrolamento do LSTM, necess\IeC {\'a}rio para o treinamento, em duas etapas. Ao todo, a rede foi treinada com 16 n\IeC {\'\i }veis de res\IeC {\'\i }duo, para gerar representa\IeC {\c c}\IeC {\~o}es de 64 bits. As conex\IeC {\~o}es verticais entre os est\IeC {\'a}gios LSTM no desenrolamento mostram o efeito da mem\IeC {\'o}ria persistente. Nessa arquitetura LSTM, cada etapa prev\IeC {\^e} a sa\IeC {\'\i }da real. \cite  {Variable2016Toderici}.\relax }}{31}{figure.caption.21}}
\newlabel{fig:toderici_ae_lstm}{{2.3}{31}{O codificador residual LSTM totalmente conectado. Esta figura mostra um desenrolamento do LSTM, necessário para o treinamento, em duas etapas. Ao todo, a rede foi treinada com 16 níveis de resíduo, para gerar representações de 64 bits. As conexões verticais entre os estágios LSTM no desenrolamento mostram o efeito da memória persistente. Nessa arquitetura LSTM, cada etapa prevê a saída real. \cite {Variable2016Toderici}.\relax }{figure.caption.21}{}}
\newlabel{eq:ae_full}{{2.11}{31}{Redes Recorrentes}{equation.2.2.11}{}}
\citation{FullResolution2017Toderici}
\citation{FullResolution2017Toderici}
\citation{FullResolution2017Toderici}
\citation{Variable2016Toderici}
\citation{FullResolution2017Toderici}
\newlabel{eq:pf}{{2.12}{32}{Redes Recorrentes}{equation.2.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The flow of the iterations in an RNN architecture. In this case, the prefix $D-$ refers to the "inverse" operation in the decoder. Also, the depth to space is a shuffle of the pixels as defined in the sub-pixel operations \cite  {FullResolution2017Toderici}.\relax }}{32}{figure.caption.22}}
\newlabel{fig:toderici3}{{2.4}{32}{The flow of the iterations in an RNN architecture. In this case, the prefix $D-$ refers to the "inverse" operation in the decoder. Also, the depth to space is a shuffle of the pixels as defined in the sub-pixel operations \cite {FullResolution2017Toderici}.\relax }{figure.caption.22}{}}
\citation{target}
\citation{boliek2000information}
\citation{SpatiallyAdaptive2018Minnen}
\citation{SpatiallyAdaptive2018Minnen}
\citation{SpatiallyAdaptive2018Minnen}
\citation{SpatiallyAdaptive2018Minnen}
\citation{SpatiallyAdaptive2018Minnen}
\citation{Priming2017Johnston}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Modelo DPCM com autocodificador}}{34}{figure.caption.23}}
\newlabel{fig:toderici8}{{2.5}{34}{Modelo DPCM com autocodificador}{figure.caption.23}{}}
\newlabel{fig:toderici7}{{2.6}{34}{}{figure.caption.24}{}}
\citation{Priming2017Johnston}
\citation{Priming2017Johnston}
\newlabel{eq:loss}{{2.13}{35}{Redes Recorrentes}{equation.2.2.13}{}}
\citation{Variable2016Toderici}
\citation{FullResolution2017Toderici}
\citation{Variable2016Toderici}
\citation{target}
\citation{SpatiallyAdaptive2018Minnen}
\citation{FullResolution2017Toderici}
\citation{Priming2017Johnston}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Tabela com resumo de codecs baseados em autocodificadores em cadeia.}}{37}{table.caption.25}}
\newlabel{table:codecs}{{2.1}{37}{Tabela com resumo de codecs baseados em autocodificadores em cadeia}{table.caption.25}{}}
\citation{End2016Balle}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Autocodificadores variacionais}{38}{section.2.3}}
\citation{End2016Balle}
\citation{End2016Balle}
\citation{End2016Balle}
\citation{End2016Balle}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces O vetor de imagem $x \in \mathbb  {R}^N$ \IeC {\'e} mapeado para o espa\IeC {\c c}o do c\IeC {\'o}digo com $y = g_a(x,\phi )$. $y$ \IeC {\'e} quantizado, produzindo um vetor $q \in \mathbb  {Z}^M $. A taxa $R$ \IeC {\'e} limitada pela entropia do setor quantizado, $H[P_q]$. Em seguida, os elementos discretos $ q $ s\IeC {\~a}o interpretados como $\mathaccentV {hat}05E{y}$ de valor cont\IeC {\'\i }nuo, que \IeC {\'e} transformado em espa\IeC {\c c}o de dados com $\mathaccentV {hat}05E{x} = g_s (\mathaccentV {hat}05E{y}, \theta )$. A compara\IeC {\c c}\IeC {\~a}o da reconstru\IeC {\c c}\IeC {\~a}o \IeC {\'e} feita em um espa\IeC {\c c}o desejado mapeado por $g_p$. Os par\IeC {\^a}metros $\phi $ e $\theta $ s\IeC {\~a}o otimizados para minimizar a distor\IeC {\c c}\IeC {\~a}o da taxa $R + \lambda D$ \cite  {End2016Balle}.\relax }}{40}{figure.caption.26}}
\newlabel{fig:balle1}{{2.7}{40}{O vetor de imagem $x \in \mathbb {R}^N$ é mapeado para o espaço do código com $y = g_a(x,\phi )$. $y$ é quantizado, produzindo um vetor $q \in \mathbb {Z}^M $. A taxa $R$ é limitada pela entropia do setor quantizado, $H[P_q]$. Em seguida, os elementos discretos $ q $ são interpretados como $\hat {y}$ de valor contínuo, que é transformado em espaço de dados com $\hat {x} = g_s (\hat {y}, \theta )$. A comparação da reconstrução é feita em um espaço desejado mapeado por $g_p$. Os parâmetros $\phi $ e $\theta $ são otimizados para minimizar a distorção da taxa $R + \lambda D$ \cite {End2016Balle}.\relax }{figure.caption.26}{}}
\citation{Variational2018Balle}
\citation{Variational2018Balle}
\citation{Variational2018Balle}
\citation{Variational2018Balle}
\citation{Variational2018Balle}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The description of the model considering the hyperprior \cite  {Variational2018Balle}.\relax }}{42}{figure.caption.27}}
\newlabel{fig:balle2}{{2.8}{42}{The description of the model considering the hyperprior \cite {Variational2018Balle}.\relax }{figure.caption.27}{}}
\citation{Autoregressive2018Minnen}
\citation{Autoregressive2018Minnen}
\citation{Variational2018Balle}
\citation{Autoregressive2018Minnen}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The context model is the autoregressive component which predicts latents from context. There's also the hyperprior, seen in earlier proposal of this architecture. Both models feed a network of entropy parameters which outputs information about the distribution of the code model to the basic VAE \cite  {Autoregressive2018Minnen}.\relax }}{43}{figure.caption.28}}
\newlabel{fig:figuras/balle3}{{2.9}{43}{The context model is the autoregressive component which predicts latents from context. There's also the hyperprior, seen in earlier proposal of this architecture. Both models feed a network of entropy parameters which outputs information about the distribution of the code model to the basic VAE \cite {Autoregressive2018Minnen}.\relax }{figure.caption.28}{}}
\citation{Lossy2017Theis}
\citation{End2016Balle}
\citation{End2016Balle}
\citation{akbari2019dsslic}
\citation{akbari2019dsslic}
\citation{zhao1901cae}
\citation{li2018learning}
\citation{ye2018progressive}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Autocodificadores baseado em GAN}{46}{section.2.4}}
\citation{santurkar2018generative}
\citation{radford2015unsupervised}
\citation{santurkar2018generative}
\citation{GAN2017Rippel}
\citation{agustsson2019generative}
\citation{mirza2014conditional}
\citation{agustsson2019generative}
\citation{agustsson2019generative}
\citation{FullResolution2017Toderici}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Metodologia}{49}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:model_1it}{{3.1}{49}{Metodologia}{equation.3.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Conv2DLSTM}}{49}{figure.caption.29}}
\newlabel{fig:convlstm}{{3.1}{49}{Conv2DLSTM}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Autocodificador desenrolado no tempo}}{50}{figure.caption.30}}
\newlabel{fig:rede_toderici}{{3.2}{50}{Autocodificador desenrolado no tempo}{figure.caption.30}{}}
\citation{Variable2016Toderici}
\newlabel{eq:bpp}{{3.2}{51}{Metodologia}{equation.3.0.2}{}}
\newlabel{eq:tc}{{3.3}{51}{Metodologia}{equation.3.0.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Par\IeC {\^a}metros das opera\IeC {\c c}\IeC {\~o}es no autocodificador.\relax }}{51}{table.caption.31}}
\newlabel{table:aerc}{{3.1}{51}{Parâmetros das operações no autocodificador.\relax }{table.caption.31}{}}
\citation{Variable2016Toderici}
\citation{FullResolution2017Toderici}
\citation{DeliverableJuly}
\citation{bib:clic}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Formato do fluxo de bits}{52}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Base de Dados}{52}{section.3.2}}
\citation{bib:div2k}
\citation{bib:ultraeye}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Histograma de todo o banco de dados\relax }}{53}{figure.caption.32}}
\newlabel{fig:histdatabase}{{3.3}{53}{Histograma de todo o banco de dados\relax }{figure.caption.32}{}}
\newlabel{list:bds}{{3.2}{53}{Base de Dados}{figure.caption.32}{}}
\citation{kodak}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Histograma da baixa entropia DB0.\relax }}{54}{figure.caption.33}}
\newlabel{fig:database0}{{3.4}{54}{Histograma da baixa entropia DB0.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Histograma do DB1 de entropia m\IeC {\'e}dia.\relax }}{55}{figure.caption.34}}
\newlabel{fig:database1}{{3.5}{55}{Histograma do DB1 de entropia média.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Histograma do DB2 de alta entropia.\relax }}{55}{figure.caption.35}}
\newlabel{fig:database2}{{3.6}{55}{Histograma do DB2 de alta entropia.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Histograma da entropia mista DB3.\relax }}{55}{figure.caption.36}}
\newlabel{fig:database3}{{3.7}{55}{Histograma da entropia mista DB3.\relax }{figure.caption.36}{}}
\citation{Priming2017Johnston}
\citation{End2016Balle}
\citation{zhao1901cae}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Histograma da entropia alta DB4.\relax }}{56}{figure.caption.37}}
\newlabel{fig:database4}{{3.8}{56}{Histograma da entropia alta DB4.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Fun\IeC {\c c}\IeC {\~a}o de Custo}{56}{section.3.3}}
\citation{Priming2017Johnston}
\citation{Priming2017Johnston}
\newlabel{eq:rdo}{{3.5}{57}{Função de Custo}{equation.3.3.5}{}}
\newlabel{eq:gain_ce}{{3.6}{57}{Função de Custo}{equation.3.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Aloca\IeC {\c c}\IeC {\~a}o din\IeC {\^a}mica de bits}{57}{section.3.4}}
\newlabel{sec:adb}{{3.4}{57}{Alocação dinâmica de bits}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Aloca\IeC {\c c}\IeC {\~a}o din\IeC {\^a}mica de bits.\relax }}{58}{figure.caption.38}}
\newlabel{fig:flux_vr}{{3.9}{58}{Alocação dinâmica de bits.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Implementa\IeC {\c c}\IeC {\~a}o}{58}{section.3.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Resultados}{60}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{enum:hiper_param}{{4}{60}{Resultados}{chapter.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o das base de dados.\relax }}{60}{table.caption.39}}
\newlabel{table:comp_datasets}{{4.1}{60}{Comparação das base de dados.\relax }{table.caption.39}{}}
\citation{FullResolution2017Toderici}
\citation{su2017}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o das bases de dados pela \IeC {\'a}rea abaixo das curvas em m\IeC {\'e}tricas de qualidade}}{61}{figure.caption.40}}
\newlabel{fig:auc1}{{4.1}{61}{Comparação das bases de dados pela área abaixo das curvas em métricas de qualidade}{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Performance do modelo em fun\IeC {\c c}\IeC {\~a}o do n\IeC {\'u}mero de \IeC {\'e}pocas de treinamento}}{62}{figure.caption.41}}
\newlabel{fig:psnr_13epocas}{{4.2}{62}{Performance do modelo em função do número de épocas de treinamento}{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o.\relax }}{62}{figure.caption.42}}
\newlabel{fig:ssim_ae_jpeg_toderici}{{4.3}{62}{Comparação.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o.\relax }}{63}{figure.caption.43}}
\newlabel{fig:msssim_ae_jpeg_toderici}{{4.4}{63}{Comparação.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o.\relax }}{63}{figure.caption.44}}
\newlabel{fig:psnr_ae_jpeg}{{4.5}{63}{Comparação.\relax }{figure.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o das fun\IeC {\c c}\IeC {\~o}es de custo.\relax }}{64}{table.caption.45}}
\newlabel{table:comp_loss}{{4.2}{64}{Comparação das funções de custo.\relax }{table.caption.45}{}}
\newlabel{enum:hiper_param2}{{4}{64}{Resultados}{equation.4.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o de codifica\IeC {\c c}\IeC {\~a}o com e sem o GZIP}}{65}{figure.caption.46}}
\newlabel{fig:gain_gzip_meida}{{4.6}{65}{Comparação de codificação com e sem o GZIP}{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Ilustra\IeC {\c c}\IeC {\~a}o da porcentagem da taxa-BD m\IeC {\'e}dia sobre a base de dados da Kodak e tendo o JPEG como \IeC {\^a}ncora. Valores cada vez mais negativos indicam que a economia m\IeC {\'e}dia de taxa de bits do nosso modelo em rela\IeC {\c c}\IeC {\~a}o ao JPEG aumenta para uma dada qualidade equivalente de PSNR na m\IeC {\'e}dia das imagens da Kodak.\relax }}{66}{figure.caption.47}}
\newlabel{fig:taxa_bd}{{4.7}{66}{Ilustração da porcentagem da taxa-BD média sobre a base de dados da Kodak e tendo o JPEG como âncora. Valores cada vez mais negativos indicam que a economia média de taxa de bits do nosso modelo em relação ao JPEG aumenta para uma dada qualidade equivalente de PSNR na média das imagens da Kodak.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Curvas de qualidade por taxa em 3 \IeC {\'e}pocas distintas}}{67}{figure.caption.48}}
\newlabel{fig:metricas_3ep}{{4.8}{67}{Curvas de qualidade por taxa em 3 épocas distintas}{figure.caption.48}{}}
\bibdata{abntex2-options,bibliography}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Ganho do GZIP por n\IeC {\'\i }vel de reconstru\IeC {\c c}\IeC {\~a}o}}{68}{figure.caption.49}}
\newlabel{fig:gain_medio_bpp}{{4.9}{68}{Ganho do GZIP por nível de reconstrução}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {chapter}{REFER\IeC {\^E}NCIAS}{68}{figure.caption.54}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces \relax }}{69}{figure.caption.50}}
\newlabel{fig:comp_gain_psnr}{{4.10}{69}{\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Em (a) temos a Kodim10 original em (b) sua vers\IeC {\~a}o reconstru\IeC {\'\i }da em 10 itera\IeC {\c c}\IeC {\~o}es. Em (c) est\IeC {\'a} representado a kodim13 e em (d) sua sua reconstru\IeC {\'\i }da em 8 itera\IeC {\c c}\IeC {\~o}es.\relax }}{69}{figure.caption.51}}
\newlabel{fig:kodim13}{{4.11}{69}{Em (a) temos a Kodim10 original em (b) sua versão reconstruída em 10 iterações. Em (c) está representado a kodim13 e em (d) sua sua reconstruída em 8 iterações.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Imagem original}}}{69}{subfigure.11.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,942 bpp. PSNR: 36,925 dB.} }}}{69}{subfigure.11.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Imagem original}}}{69}{subfigure.11.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,987 bpp. PSNR: 26,357 dB.} }}}{69}{subfigure.11.4}}
\newlabel{fig:artf}{{\caption@xref {fig:artf}{ on input line 246}}{70}{Resultados}{subfigure.4.12.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Surgimento de artefatos de blocos em modelos baseados em aloca\IeC {\c c}\IeC {\~a}o de bits.}}{70}{figure.caption.52}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Imagem original}}}{70}{subfigure.12.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,741 bpp, PSNR: 32,190dB, SSIM: 0,8675.} }}}{70}{subfigure.12.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Imagem reconstru\IeC {\'\i }da. Taxa: 0,782 bpp, PSNR: 32,687dB, SSIM: 0,8939}}}{70}{subfigure.12.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,754 bpp, PSNR: 31,621, SSIM:0,8985 } }}}{70}{subfigure.12.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces \relax }}{70}{figure.caption.53}}
\newlabel{fig:comp_vr}{{4.13}{70}{\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces \relax }}{71}{figure.caption.54}}
\newlabel{fig:comp_codecs}{{4.14}{71}{\relax }{figure.caption.54}{}}
\bibcite{Variable2016Toderici}{1}
\bibciteEXPL{Variable2016Toderici}{Toderici et al.}
\bibciteIMPL{Variable2016Toderici}{TODERICI et al.}
\bibciteYEAR{Variable2016Toderici}{2015}
\bibcite{FullResolution2017Toderici}{2}
\bibciteEXPL{FullResolution2017Toderici}{Toderici et al.}
\bibciteIMPL{FullResolution2017Toderici}{TODERICI et al.}
\bibciteYEAR{FullResolution2017Toderici}{2017}
\bibcite{End2016Balle}{3}
\bibciteEXPL{End2016Balle}{Ball{\'{e}}, Laparra e Simoncelli}
\bibciteIMPL{End2016Balle}{BALL{\'{E}}; LAPARRA; SIMONCELLI}
\bibciteYEAR{End2016Balle}{2016}
\bibcite{Variational2018Balle}{4}
\bibciteEXPL{Variational2018Balle}{Ball{\'{e}} et al.}
\bibciteIMPL{Variational2018Balle}{BALL{\'{E}} et al.}
\bibciteYEAR{Variational2018Balle}{2018}
\bibcite{shalev2014understanding}{5}
\bibciteEXPL{shalev2014understanding}{Shalev-Shwartz e Ben-David}
\bibciteIMPL{shalev2014understanding}{SHALEV-SHWARTZ; BEN-DAVID}
\bibciteYEAR{shalev2014understanding}{2014}
\bibcite{FrancoisDeepLearning}{6}
\bibciteEXPL{FrancoisDeepLearning}{CHOLLET}
\bibciteIMPL{FrancoisDeepLearning}{CHOLLET}
\bibciteYEAR{FrancoisDeepLearning}{2018}
\bibcite{Haykin}{7}
\bibciteEXPL{Haykin}{Haykin}
\bibciteIMPL{Haykin}{HAYKIN}
\bibciteYEAR{Haykin}{2001}
\bibcite{lecun2015deep}{8}
\bibciteEXPL{lecun2015deep}{LeCun, Bengio e Hinton}
\bibciteIMPL{lecun2015deep}{LECUN; BENGIO; HINTON}
\bibciteYEAR{lecun2015deep}{2015}
\bibcite{bengio2009learning}{9}
\bibciteEXPL{bengio2009learning}{Bengio et al.}
\bibciteIMPL{bengio2009learning}{BENGIO et al.}
\bibciteYEAR{bengio2009learning}{2009}
\bibcite{Mitchell}{10}
\bibciteEXPL{Mitchell}{Mitchell}
\bibciteIMPL{Mitchell}{MITCHELL}
\bibciteYEAR{Mitchell}{1997}
\bibcite{mcculloch1943logical}{11}
\bibciteEXPL{mcculloch1943logical}{McCulloch e Pitts}
\bibciteIMPL{mcculloch1943logical}{MCCULLOCH; PITTS}
\bibciteYEAR{mcculloch1943logical}{1943}
\bibcite{Goodfellow2016}{12}
\bibciteEXPL{Goodfellow2016}{Goodfellow, Bengio e Courville}
\bibciteIMPL{Goodfellow2016}{GOODFELLOW; BENGIO; COURVILLE}
\bibciteYEAR{Goodfellow2016}{2016}
\bibcite{nair2010rectified}{13}
\bibciteEXPL{nair2010rectified}{Nair e Hinton}
\bibciteIMPL{nair2010rectified}{NAIR; HINTON}
\bibciteYEAR{nair2010rectified}{2010}
\bibcite{xu2015empirical}{14}
\bibciteEXPL{xu2015empirical}{Xu et al.}
\bibciteIMPL{xu2015empirical}{XU et al.}
\bibciteYEAR{xu2015empirical}{2015}
\bibcite{rumelhart1988learning}{15}
\bibciteEXPL{rumelhart1988learning}{Rumelhart et al.}
\bibciteIMPL{rumelhart1988learning}{RUMELHART et al.}
\bibciteYEAR{rumelhart1988learning}{1988}
\bibcite{hornik1989multilayer}{16}
\bibciteEXPL{hornik1989multilayer}{Hornik, Stinchcombe e White}
\bibciteIMPL{hornik1989multilayer}{HORNIK; STINCHCOMBE; WHITE}
\bibciteYEAR{hornik1989multilayer}{1989}
\bibcite{leshno1993multilayer}{17}
\bibciteEXPL{leshno1993multilayer}{Leshno et al.}
\bibciteIMPL{leshno1993multilayer}{LESHNO et al.}
\bibciteYEAR{leshno1993multilayer}{1993}
\bibcite{geron2017hands}{18}
\bibciteEXPL{geron2017hands}{G{\'e}ron}
\bibciteIMPL{geron2017hands}{G{\'E}RON}
\bibciteYEAR{geron2017hands}{2017}
\bibcite{sumit}{19}
\bibciteEXPL{sumit}{Saha}
\bibciteIMPL{sumit}{SAHA}
\bibciteYEAR{sumit}{}
\bibcite{rumelhart1986learning}{20}
\bibciteEXPL{rumelhart1986learning}{Rumelhart, Hinton e Williams}
\bibciteIMPL{rumelhart1986learning}{RUMELHART; HINTON; WILLIAMS}
\bibciteYEAR{rumelhart1986learning}{1986}
\bibcite{ketkar2017deep}{21}
\bibciteEXPL{ketkar2017deep}{Ketkar et al.}
\bibciteIMPL{ketkar2017deep}{KETKAR et al.}
\bibciteYEAR{ketkar2017deep}{2017}
\bibcite{lstm}{22}
\bibciteEXPL{lstm}{Hochreiter e Schmidhuber}
\bibciteIMPL{lstm}{HOCHREITER; SCHMIDHUBER}
\bibciteYEAR{lstm}{1997}
\bibcite{Olah}{23}
\bibciteEXPL{Olah}{Olah}
\bibciteIMPL{Olah}{OLAH}
\bibciteYEAR{Olah}{}
\bibcite{xingjian2015convolutional}{24}
\bibciteEXPL{xingjian2015convolutional}{Xingjian et al.}
\bibciteIMPL{xingjian2015convolutional}{XINGJIAN et al.}
\bibciteYEAR{xingjian2015convolutional}{2015}
\bibcite{ng2011sparse}{25}
\bibciteEXPL{ng2011sparse}{Ng et al.}
\bibciteIMPL{ng2011sparse}{NG et al.}
\bibciteYEAR{ng2011sparse}{2011}
\bibcite{kullback1951information}{26}
\bibciteEXPL{kullback1951information}{Kullback e Leibler}
\bibciteIMPL{kullback1951information}{KULLBACK; LEIBLER}
\bibciteYEAR{kullback1951information}{1951}
\bibcite{kingma2013auto}{27}
\bibciteEXPL{kingma2013auto}{Kingma e Welling}
\bibciteIMPL{kingma2013auto}{KINGMA; WELLING}
\bibciteYEAR{kingma2013auto}{2013}
\bibcite{rezende2014stochastic}{28}
\bibciteEXPL{rezende2014stochastic}{Rezende, Mohamed e Wierstra}
\bibciteIMPL{rezende2014stochastic}{REZENDE; MOHAMED; WIERSTRA}
\bibciteYEAR{rezende2014stochastic}{2014}
\bibcite{klys2018learning}{29}
\bibciteEXPL{klys2018learning}{Klys, Snell e Zemel}
\bibciteIMPL{klys2018learning}{KLYS; SNELL; ZEMEL}
\bibciteYEAR{klys2018learning}{2018}
\bibcite{rolinek2019variational}{30}
\bibciteEXPL{rolinek2019variational}{Rolinek, Zietlow e Martius}
\bibciteIMPL{rolinek2019variational}{ROLINEK; ZIETLOW; MARTIUS}
\bibciteYEAR{rolinek2019variational}{2019}
\bibcite{sonderby2016ladder}{31}
\bibciteEXPL{sonderby2016ladder}{S{\o }nderby et al.}
\bibciteIMPL{sonderby2016ladder}{S{\O }NDERBY et al.}
\bibciteYEAR{sonderby2016ladder}{2016}
\bibcite{gonzalez2009processamento}{32}
\bibciteEXPL{gonzalez2009processamento}{Gonzalez e Woods}
\bibciteIMPL{gonzalez2009processamento}{GONZALEZ; WOODS}
\bibciteYEAR{gonzalez2009processamento}{2009}
\bibcite{sayood2017introduction}{33}
\bibciteEXPL{sayood2017introduction}{Sayood}
\bibciteIMPL{sayood2017introduction}{SAYOOD}
\bibciteYEAR{sayood2017introduction}{2017}
\bibcite{marques1999processamento}{34}
\bibciteEXPL{marques1999processamento}{Filho e Neto}
\bibciteIMPL{marques1999processamento}{FILHO; NETO}
\bibciteYEAR{marques1999processamento}{1999}
\bibcite{richardson2010h}{35}
\bibciteEXPL{richardson2010h}{Richardson}
\bibciteIMPL{richardson2010h}{RICHARDSON}
\bibciteYEAR{richardson2010h}{2010}
\bibcite{salomon2007data}{36}
\bibciteEXPL{salomon2007data}{Salomon}
\bibciteIMPL{salomon2007data}{SALOMON}
\bibciteYEAR{salomon2007data}{2007}
\bibcite{hore2010image}{37}
\bibciteEXPL{hore2010image}{Hore e Ziou}
\bibciteIMPL{hore2010image}{HORE; ZIOU}
\bibciteYEAR{hore2010image}{2010}
\bibcite{wang2003multiscale}{38}
\bibciteEXPL{wang2003multiscale}{Wang, Simoncelli e Bovik}
\bibciteIMPL{wang2003multiscale}{WANG; SIMONCELLI; BOVIK}
\bibciteYEAR{wang2003multiscale}{2003}
\bibcite{gersho2012vector}{39}
\bibciteEXPL{gersho2012vector}{Gersho e Gray}
\bibciteIMPL{gersho2012vector}{GERSHO; GRAY}
\bibciteYEAR{gersho2012vector}{2012}
\bibcite{santurkar2018generative}{40}
\bibciteEXPL{santurkar2018generative}{Santurkar, Budden e Shavit}
\bibciteIMPL{santurkar2018generative}{SANTURKAR; BUDDEN; SHAVIT}
\bibciteYEAR{santurkar2018generative}{2018}
\bibcite{Jiang1999}{41}
\bibciteEXPL{Jiang1999}{Jiang}
\bibciteIMPL{Jiang1999}{JIANG}
\bibciteYEAR{Jiang1999}{1999}
\bibcite{akbari2019dsslic}{42}
\bibciteEXPL{akbari2019dsslic}{Akbari, Liang e Han}
\bibciteIMPL{akbari2019dsslic}{AKBARI; LIANG; HAN}
\bibciteYEAR{akbari2019dsslic}{2019}
\bibcite{nie2010efficient}{43}
\bibciteEXPL{nie2010efficient}{Nie et al.}
\bibciteIMPL{nie2010efficient}{NIE et al.}
\bibciteYEAR{nie2010efficient}{2010}
\bibcite{target}{44}
\bibciteEXPL{target}{Covell et al.}
\bibciteIMPL{target}{COVELL et al.}
\bibciteYEAR{target}{2017}
\bibcite{boliek2000information}{45}
\bibciteEXPL{boliek2000information}{Boliek, Christopoulos e Majani}
\bibciteIMPL{boliek2000information}{BOLIEK; CHRISTOPOULOS; MAJANI}
\bibciteYEAR{boliek2000information}{2000}
\bibcite{SpatiallyAdaptive2018Minnen}{46}
\bibciteEXPL{SpatiallyAdaptive2018Minnen}{Minnen et al.}
\bibciteIMPL{SpatiallyAdaptive2018Minnen}{MINNEN et al.}
\bibciteYEAR{SpatiallyAdaptive2018Minnen}{2017}
\bibcite{Priming2017Johnston}{47}
\bibciteEXPL{Priming2017Johnston}{Johnston et al.}
\bibciteIMPL{Priming2017Johnston}{JOHNSTON et al.}
\bibciteYEAR{Priming2017Johnston}{2017}
\bibcite{Lossy2017Theis}{48}
\bibciteEXPL{Lossy2017Theis}{Theis et al.}
\bibciteIMPL{Lossy2017Theis}{THEIS et al.}
\bibciteYEAR{Lossy2017Theis}{2017}
\bibcite{zhao1901cae}{49}
\bibciteEXPL{zhao1901cae}{Zhao e Liao}
\bibciteIMPL{zhao1901cae}{ZHAO; LIAO}
\bibciteYEAR{zhao1901cae}{1901}
\bibcite{li2018learning}{50}
\bibciteEXPL{li2018learning}{Li et al.}
\bibciteIMPL{li2018learning}{LI et al.}
\bibciteYEAR{li2018learning}{2018}
\bibcite{ye2018progressive}{51}
\bibciteEXPL{ye2018progressive}{Ye et al.}
\bibciteIMPL{ye2018progressive}{YE et al.}
\bibciteYEAR{ye2018progressive}{2018}
\bibcite{radford2015unsupervised}{52}
\bibciteEXPL{radford2015unsupervised}{Radford, Metz e Chintala}
\bibciteIMPL{radford2015unsupervised}{RADFORD; METZ; CHINTALA}
\bibciteYEAR{radford2015unsupervised}{2015}
\bibcite{GAN2017Rippel}{53}
\bibciteEXPL{GAN2017Rippel}{Rippel e Bourdev}
\bibciteIMPL{GAN2017Rippel}{RIPPEL; BOURDEV}
\bibciteYEAR{GAN2017Rippel}{2017}
\bibcite{agustsson2019generative}{54}
\bibciteEXPL{agustsson2019generative}{Agustsson et al.}
\bibciteIMPL{agustsson2019generative}{AGUSTSSON et al.}
\bibciteYEAR{agustsson2019generative}{2019}
\bibcite{mirza2014conditional}{55}
\bibciteEXPL{mirza2014conditional}{Mirza e Osindero}
\bibciteIMPL{mirza2014conditional}{MIRZA; OSINDERO}
\bibciteYEAR{mirza2014conditional}{2014}
\bibcite{DeliverableJuly}{56}
\bibciteEXPL{DeliverableJuly}{Hung et al.}
\bibciteIMPL{DeliverableJuly}{HUNG et al.}
\bibciteYEAR{DeliverableJuly}{2019}
\bibcite{bib:clic}{57}
\bibciteEXPL{bib:clic}{Dataset\ldots  }
\bibciteIMPL{bib:clic}{DATASET\ldots  }
\bibciteYEAR{bib:clic}{}
\bibcite{bib:div2k}{58}
\bibciteEXPL{bib:div2k}{Agustsson e Timofte}
\bibciteIMPL{bib:div2k}{AGUSTSSON; TIMOFTE}
\bibciteYEAR{bib:div2k}{2017}
\bibcite{bib:ultraeye}{59}
\bibciteEXPL{bib:ultraeye}{Nemoto et al.}
\bibciteIMPL{bib:ultraeye}{NEMOTO et al.}
\bibciteYEAR{bib:ultraeye}{2014}
\bibcite{kodak}{60}
\bibciteEXPL{kodak}{Kodak\ldots  }
\bibciteIMPL{kodak}{KODAK\ldots  }
\bibciteYEAR{kodak}{2019}
\bibcite{su2017}{61}
\bibciteEXPL{su2017}{Su}
\bibciteIMPL{su2017}{SU}
\bibciteYEAR{su2017}{2017}
\newlabel{LastPage}{{}{75}{}{page.75}{}}
\bibstyle{abntex2-num}
\pagesLTS@ifcounter{pagesLTS.arabic.1.local.cnt}
\setcounter{pagesLTS.arabic.1.local.cnt}{75}
\gdef\pagesLTS.lastpage{75}
\setcounter{pagesLTS.pagenr}{93}
\newlabel{VeryLastPage}{{}{75}{}{page.75}{}}
\newlabel{pagesLTS.arabic.1}{{}{75}{}{page.75}{}}
\newlabel{pagesLTS.arabic.1.local}{{}{75}{}{page.75}{}}
\newlabel{pagesLTS.arabic}{{}{75}{}{page.75}{}}
\newlabel{pagesLTS.arabic.local}{{}{75}{}{page.75}{}}
\newlabel{LastPages}{{}{93}{}{page.75}{}}
