\select@language {portuguese}
\select@language {portuguese}
\select@language {portuguese}
\select@language {portuguese}
\select@language {portuguese}
\select@language {english}
\select@language {portuguese}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Paradigmas de programa\IeC {\c c}\IeC {\~a}o}}{1}{figure.caption.6}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Principais componentes em uma \acrshort {rna}}}{2}{figure.caption.7}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Modelo do neur\IeC {\^o}nio matem\IeC {\'a}tico.}}{3}{figure.caption.8}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Superf\IeC {\'\i }cie de erro para o aprendizado dos pesos de um neur\IeC {\^o}nio simples.}}{6}{figure.caption.10}
\contentsline {figure}{\numberline {1.5}{\ignorespaces Rede \textit {feedforward} de 2 camadas}}{9}{figure.caption.11}
\contentsline {figure}{\numberline {1.6}{\ignorespaces Convolu\IeC {\c c}\IeC {\~a}o 2D em um mapa de recurso.}}{11}{figure.caption.12}
\contentsline {figure}{\numberline {1.7}{\ignorespaces Convolu\IeC {\c c}\IeC {\~a}o 2D em tr\IeC {\^e}s mapa de recurso.}}{12}{figure.caption.13}
\contentsline {figure}{\numberline {1.8}{\ignorespaces Arquitetura RNR.}}{12}{figure.caption.14}
\contentsline {figure}{\numberline {1.9}{\ignorespaces C\IeC {\'e}lula \acrshort {lstm} no tempo}}{14}{figure.caption.15}
\contentsline {figure}{\numberline {1.10}{\ignorespaces Compress\IeC {\~a}o e reconstru\IeC {\c c}\IeC {\~a}o sem perdas.}}{19}{figure.caption.16}
\contentsline {figure}{\numberline {1.11}{\ignorespaces Espa\IeC {\c c}o tridimensional de cores RGB.}}{21}{figure.caption.17}
\contentsline {figure}{\numberline {1.12}{\ignorespaces Padr\IeC {\~a}o \acrshort {jpeg}}}{23}{figure.caption.18}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Um autoencoder residual totalmente conectado com fun\IeC {\c c}\IeC {\~a}o de ativa\IeC {\c c}\IeC {\~a}o $\qopname \relax o{tanh}$. Esta figura representa uma arquitetura de duas itera\IeC {\c c}\IeC {\~o}es. As primeiras itera\IeC {\c c}\IeC {\~o}es codificam a imagem original. Os res\IeC {\'\i }duos da reconstru\IeC {\c c}\IeC {\~a}o s\IeC {\~a}o passados para a segunda itera\IeC {\c c}\IeC {\~a}o. Cada n\IeC {\'\i }vel de itera\IeC {\c c}\IeC {\~a}o produz 4 bits \cite {Variable2016Toderici}.\relax }}{29}{figure.caption.19}
\contentsline {figure}{\numberline {2.2}{\ignorespaces O codificador residual convolucional/deconvolucional. As camadas convolucionais s\IeC {\~a}o representadas como ret\IeC {\^a}ngulos pontiagudos, enquanto as camadas deconvolucionais s\IeC {\~a}o representadas como ret\IeC {\^a}ngulos arredondados. \cite {Variable2016Toderici}.\relax }}{30}{figure.caption.20}
\contentsline {figure}{\numberline {2.3}{\ignorespaces O codificador residual LSTM totalmente conectado. Esta figura mostra um desenrolamento do LSTM, necess\IeC {\'a}rio para o treinamento, em duas etapas. Ao todo, a rede foi treinada com 16 n\IeC {\'\i }veis de res\IeC {\'\i }duo, para gerar representa\IeC {\c c}\IeC {\~o}es de 64 bits. As conex\IeC {\~o}es verticais entre os est\IeC {\'a}gios LSTM no desenrolamento mostram o efeito da mem\IeC {\'o}ria persistente. Nessa arquitetura LSTM, cada etapa prev\IeC {\^e} a sa\IeC {\'\i }da real. \cite {Variable2016Toderici}.\relax }}{31}{figure.caption.21}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The flow of the iterations in an RNN architecture. In this case, the prefix $D-$ refers to the "inverse" operation in the decoder. Also, the depth to space is a shuffle of the pixels as defined in the sub-pixel operations \cite {FullResolution2017Toderici}.\relax }}{32}{figure.caption.22}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Modelo DPCM com autocodificador}}{34}{figure.caption.23}
\contentsline {figure}{\numberline {2.7}{\ignorespaces O vetor de imagem $x \in \mathbb {R}^N$ \IeC {\'e} mapeado para o espa\IeC {\c c}o do c\IeC {\'o}digo com $y = g_a(x,\phi )$. $y$ \IeC {\'e} quantizado, produzindo um vetor $q \in \mathbb {Z}^M $. A taxa $R$ \IeC {\'e} limitada pela entropia do setor quantizado, $H[P_q]$. Em seguida, os elementos discretos $ q $ s\IeC {\~a}o interpretados como $\mathaccentV {hat}05E{y}$ de valor cont\IeC {\'\i }nuo, que \IeC {\'e} transformado em espa\IeC {\c c}o de dados com $\mathaccentV {hat}05E{x} = g_s (\mathaccentV {hat}05E{y}, \theta )$. A compara\IeC {\c c}\IeC {\~a}o da reconstru\IeC {\c c}\IeC {\~a}o \IeC {\'e} feita em um espa\IeC {\c c}o desejado mapeado por $g_p$. Os par\IeC {\^a}metros $\phi $ e $\theta $ s\IeC {\~a}o otimizados para minimizar a distor\IeC {\c c}\IeC {\~a}o da taxa $R + \lambda D$ \cite {End2016Balle}.\relax }}{40}{figure.caption.26}
\contentsline {figure}{\numberline {2.8}{\ignorespaces The description of the model considering the hyperprior \cite {Variational2018Balle}.\relax }}{42}{figure.caption.27}
\contentsline {figure}{\numberline {2.9}{\ignorespaces The context model is the autoregressive component which predicts latents from context. There's also the hyperprior, seen in earlier proposal of this architecture. Both models feed a network of entropy parameters which outputs information about the distribution of the code model to the basic VAE \cite {Autoregressive2018Minnen}.\relax }}{43}{figure.caption.28}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Conv2DLSTM}}{49}{figure.caption.29}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Autocodificador desenrolado no tempo}}{50}{figure.caption.30}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Histograma de todo o banco de dados\relax }}{53}{figure.caption.32}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Histograma da baixa entropia DB0.\relax }}{54}{figure.caption.33}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Histograma do DB1 de entropia m\IeC {\'e}dia.\relax }}{55}{figure.caption.34}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Histograma do DB2 de alta entropia.\relax }}{55}{figure.caption.35}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Histograma da entropia mista DB3.\relax }}{55}{figure.caption.36}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Histograma da entropia alta DB4.\relax }}{56}{figure.caption.37}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Aloca\IeC {\c c}\IeC {\~a}o din\IeC {\^a}mica de bits.\relax }}{58}{figure.caption.38}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o das bases de dados pela \IeC {\'a}rea abaixo das curvas em m\IeC {\'e}tricas de qualidade}}{61}{figure.caption.40}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Performance do modelo em fun\IeC {\c c}\IeC {\~a}o do n\IeC {\'u}mero de \IeC {\'e}pocas de treinamento}}{62}{figure.caption.41}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o.\relax }}{62}{figure.caption.42}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o.\relax }}{63}{figure.caption.43}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o.\relax }}{63}{figure.caption.44}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Compara\IeC {\c c}\IeC {\~a}o de codifica\IeC {\c c}\IeC {\~a}o com e sem o GZIP}}{65}{figure.caption.46}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Ilustra\IeC {\c c}\IeC {\~a}o da porcentagem da taxa-BD m\IeC {\'e}dia sobre a base de dados da Kodak e tendo o JPEG como \IeC {\^a}ncora. Valores cada vez mais negativos indicam que a economia m\IeC {\'e}dia de taxa de bits do nosso modelo em rela\IeC {\c c}\IeC {\~a}o ao JPEG aumenta para uma dada qualidade equivalente de PSNR na m\IeC {\'e}dia das imagens da Kodak.\relax }}{66}{figure.caption.47}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Curvas de qualidade por taxa em 3 \IeC {\'e}pocas distintas}}{67}{figure.caption.48}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Ganho do GZIP por n\IeC {\'\i }vel de reconstru\IeC {\c c}\IeC {\~a}o}}{68}{figure.caption.49}
\contentsline {figure}{\numberline {4.10}{\ignorespaces \relax }}{69}{figure.caption.50}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Em (a) temos a Kodim10 original em (b) sua vers\IeC {\~a}o reconstru\IeC {\'\i }da em 10 itera\IeC {\c c}\IeC {\~o}es. Em (c) est\IeC {\'a} representado a kodim13 e em (d) sua sua reconstru\IeC {\'\i }da em 8 itera\IeC {\c c}\IeC {\~o}es.\relax }}{69}{figure.caption.51}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Imagem original}}}{69}{subfigure.11.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,942 bpp. PSNR: 36,925 dB.} }}}{69}{subfigure.11.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Imagem original}}}{69}{subfigure.11.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,987 bpp. PSNR: 26,357 dB.} }}}{69}{subfigure.11.4}
\contentsline {figure}{\numberline {4.12}{\ignorespaces Surgimento de artefatos de blocos em modelos baseados em aloca\IeC {\c c}\IeC {\~a}o de bits.}}{70}{figure.caption.52}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Imagem original}}}{70}{subfigure.12.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,741 bpp, PSNR: 32,190dB, SSIM: 0,8675.} }}}{70}{subfigure.12.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Imagem reconstru\IeC {\'\i }da. Taxa: 0,782 bpp, PSNR: 32,687dB, SSIM: 0,8939}}}{70}{subfigure.12.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {{Imagem reconstru\IeC {\'\i }da. Taxa: 0,754 bpp, PSNR: 31,621, SSIM:0,8985 } }}}{70}{subfigure.12.4}
\contentsline {figure}{\numberline {4.13}{\ignorespaces \relax }}{70}{figure.caption.53}
\contentsline {figure}{\numberline {4.14}{\ignorespaces \relax }}{71}{figure.caption.54}
